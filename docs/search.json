[
  {
    "objectID": "block_1/511_python/511_python.html",
    "href": "block_1/511_python/511_python.html",
    "title": "Python Basics",
    "section": "",
    "text": "int, float, str, bool, list, tuple, dict, set, None\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(text)\nLength of text\n\n\ntext.upper()\nUppercase\n\n\ntext.lower()\nLowercase\n\n\ntext.capitalize()\nCapitalize first letter\n\n\ntext.title()\nCapitalize first letter of each word\n\n\ntext.strip()\nRemove leading and trailing whitespace\n\n\ntext.split(' ')\nSplit string into list of words, using ’ ’ as delimiter. Default delimiter is ’ ’.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(list)\nLength of list\n\n\nlist.append(item)\nAppend item to end of list\n\n\nlist.insert(index, item)\nInsert item at index\n\n\nlist.pop(n)\nRemove and return item at index n. Default n is -1\n\n\nsorted(list)\nReturns a new sorted list without modifying original list\n\n\nlist.sort()\nSort list in ascending order. To sort in descending order, use list.sort(reverse=True). (edit original list)\n\n\nlist.reverse()\nReverse list in place (edit original list)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nCreate dictionary\ndict = {'key1': 'value1', 'key2': 'value2'} or dict = dict(key1='value1', key2='value2')\n\n\nCreate empty dictionary\ndict = {} or dict = dict()\n\n\ndict['key']\nGet value of key\n\n\ndict.get('key', default)\nGet value of key, if key does not exist, return default value\n\n\ndict.pop('key')\nRemove and return value of key\n\n\ndict.keys()\nReturn list of keys\n\n\ndict.values()\nReturn list of values\n\n\ndict.items()\nReturn list of tuples (key, value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nList\nTuple\nDictionary (dict)\nSet\n\n\n\n\nMutable\nYes\nNo\nYes\nYes\n\n\nOrdered\nYes\nYes\nNo\nNo\n\n\nIndexing\nBy index (0-based)\nBy index (0-based)\nBy key\nNo\n\n\nDuplicates\nAllowed\nAllowed\nKeys must be unique\nDuplicates not allowed\n\n\nModification\nCan change elements\nCannot change elements\nValues can be updated\nElements can be added/removed\n\n\nSyntax\nSquare brackets []\nParentheses ()\nCurly braces {}\nCurly braces {}\n\n\nUse Case\nWhen order matters\nWhen data should not change\nMapping keys to values\nFor unique values and set operations\n\n\nExample\n[1, 2, 'three']\n(1, 2, 'three')\n{'name': 'Alice', 'age': 30}\n{'apple', 'banana', 'cherry'}\n\n\n\n\n\n\n\n    # syntax: [expression for item in list]\n    # syntax: [expression for item in list if condition]\n    # syntax: [expression if condition else other_expression for item in list]\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = [len(word) for word in words] # [3, 4, 2, 5]\n    x = [word for word in words if len(word) &gt; 2] # [\"the\", \"list\", \"words\"]\n    x = [word if len(word) &gt; 2 else \"short\" for word in words] # [\"the\", \"short\", \"short\", \"words\"]\n\n\n\n    # syntax: value_if_true if condition else value_if_false\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = \"long list\" if len(words) &gt; 10 else \"short list\"\n\n\n\n    try:\n        # code that might raise an exception\n    except:\n        # code to handle exception\n        raise TypeError(\"Error message\") # raise exception\n\n\n\n    def function_name(arg1, arg2, arg3=default_value): # default values are optional\n        # code\n        return value\n\n    def function_name(*args): # takes in multiple arguments\n        for arg in args:\n            # code\n        return value\n\nside effects: If a function does anything other than returning a value, it is said to have side effects. An example of this is when a function changes the variables passed into it, or when a function prints something to the screen.\n\n\n\n    # Syntax: lambda arg1, arg2: expression\n    lambda x: x+1\n\n\n\n    # NumPy/SciPy style\n    def repeat_string(s, n=2):\n        \"\"\"\n        Repeat the string s, n times.\n\n        Parameters\n        ----------\n        s : str\n            the string\n        n : int, optional\n            the number of times, by default = 2\n\n        Returns\n        -------\n        str\n            the repeated string\n\n        Examples\n        --------\n        &gt;&gt;&gt; repeat_string(\"Blah\", 3)\n        \"BlahBlahBlah\"\n        \"\"\"\n        return s * n\n\n\n\n    def repeat_string(s: str, n: int = 2) -&gt; str:\n        return s * n\n\n\n\n\n    class ClassName:\n        def __init__(self, arg1, arg2):\n            # code\n        def method_name(self, arg1, arg2):\n            # code\n        @classmethod\n        def other_method_name(cls, arg1, arg2):\n            # classmethod is used to create factory methods, aka other ways to create objects\n            # code\n\n    # Inheritance\n    class SubClassName(ClassName):\n        def __init__(self, arg1, arg2):\n            super().__init__(arg1, arg2)\n            # code\n        def method2_name(self, arg1, arg2):\n            # code\n\n\n    class ClassName:\n        static_var = 0\n\n        @staticmethod\n        def method_name(arg1, arg2):\n            # code\n            ClassName.static_var += 1\n            return ClassName.static_var\n        @staticmethod\n        def reset_static_var():\n            ClassName.static_var = 0\n\n\n\n    class TestVector(unittest.TestCase):\n        def test_str1(self):\n            v1 = Vector(1, 2, 3)\n            self.assertIn(\"Vector = [1, 2, 3]\", v1.__str__())\n            self.assertEqual(len(v1.elements), 3)\n\n        def test_str2(self):\n            v1 = Vector(500)\n            self.assertIn(\"Vector = [500]\", v1.__str__())\n\n\n    TestVector = unittest.main(argv=[\"\"], verbosity=0, exit=False)\n    assert TestVector.result.wasSuccessful()\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nRule/Convention\nExample\n\n\n\n\nIndentation\nUse 4 spaces per indentation level\nif x: # four spaces here\n\n\n\nUse spaces around operators and after commas\na = b + c, d = e + f\n\n\nMaximum Line Length\nLimit all lines to a maximum of 79 characters for code, 72 for comments and docstrings\n\n\n\nImports\nAlways put imports at the top of the file\nimport os\n\n\n\nGroup imports: standard, third-party, local\nimport os import numpy as np from . import my_module\n\n\n\nUse absolute imports\nfrom my_pkg import module\n\n\nWhitespace\nAvoid extraneous whitespace\nspam(ham[1], {eggs: 2})\n\n\n\nUse blank lines to separate functions, classes, blocks of code inside functions\n\n\n\nComments\nComments should be complete sentences\n# This is a complete sentence.\n\n\n\nUse inline comments sparingly\nx = x + 1 # Increment x\n\n\nNaming Conventions\nFunction names should be lowercase with underscores\nmy_function()\n\n\n\nClass names should use CapWords convention\nMyClass\n\n\n\nConstants should be in all capital letters\nCONSTANT_NAME\n\n\nString Quotes\nUse double quotes for docstrings and single quotes for everything else when you start a project (or adhere to the project’s conventions)\n‘string’, “““docstring”“”\n\n\nExpressions and Statements\nDon’t use semicolons to terminate statements\nx = 1 (not x = 1;)\n\n\n\nUse is for identity comparisons and == for value comparisons\nif x is None\n\n\nOther Recommendations\nUse built-in types like list, dict instead of List, Dict from the typing module for simple use-cases\ndef func(a: list) -&gt; None:"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-types",
    "href": "block_1/511_python/511_python.html#data-types",
    "title": "Python Basics",
    "section": "",
    "text": "int, float, str, bool, list, tuple, dict, set, None\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(text)\nLength of text\n\n\ntext.upper()\nUppercase\n\n\ntext.lower()\nLowercase\n\n\ntext.capitalize()\nCapitalize first letter\n\n\ntext.title()\nCapitalize first letter of each word\n\n\ntext.strip()\nRemove leading and trailing whitespace\n\n\ntext.split(' ')\nSplit string into list of words, using ’ ’ as delimiter. Default delimiter is ’ ’.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(list)\nLength of list\n\n\nlist.append(item)\nAppend item to end of list\n\n\nlist.insert(index, item)\nInsert item at index\n\n\nlist.pop(n)\nRemove and return item at index n. Default n is -1\n\n\nsorted(list)\nReturns a new sorted list without modifying original list\n\n\nlist.sort()\nSort list in ascending order. To sort in descending order, use list.sort(reverse=True). (edit original list)\n\n\nlist.reverse()\nReverse list in place (edit original list)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nCreate dictionary\ndict = {'key1': 'value1', 'key2': 'value2'} or dict = dict(key1='value1', key2='value2')\n\n\nCreate empty dictionary\ndict = {} or dict = dict()\n\n\ndict['key']\nGet value of key\n\n\ndict.get('key', default)\nGet value of key, if key does not exist, return default value\n\n\ndict.pop('key')\nRemove and return value of key\n\n\ndict.keys()\nReturn list of keys\n\n\ndict.values()\nReturn list of values\n\n\ndict.items()\nReturn list of tuples (key, value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nList\nTuple\nDictionary (dict)\nSet\n\n\n\n\nMutable\nYes\nNo\nYes\nYes\n\n\nOrdered\nYes\nYes\nNo\nNo\n\n\nIndexing\nBy index (0-based)\nBy index (0-based)\nBy key\nNo\n\n\nDuplicates\nAllowed\nAllowed\nKeys must be unique\nDuplicates not allowed\n\n\nModification\nCan change elements\nCannot change elements\nValues can be updated\nElements can be added/removed\n\n\nSyntax\nSquare brackets []\nParentheses ()\nCurly braces {}\nCurly braces {}\n\n\nUse Case\nWhen order matters\nWhen data should not change\nMapping keys to values\nFor unique values and set operations\n\n\nExample\n[1, 2, 'three']\n(1, 2, 'three')\n{'name': 'Alice', 'age': 30}\n{'apple', 'banana', 'cherry'}"
  },
  {
    "objectID": "block_1/511_python/511_python.html#inline-for-loop",
    "href": "block_1/511_python/511_python.html#inline-for-loop",
    "title": "Python Basics",
    "section": "",
    "text": "# syntax: [expression for item in list]\n    # syntax: [expression for item in list if condition]\n    # syntax: [expression if condition else other_expression for item in list]\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = [len(word) for word in words] # [3, 4, 2, 5]\n    x = [word for word in words if len(word) &gt; 2] # [\"the\", \"list\", \"words\"]\n    x = [word if len(word) &gt; 2 else \"short\" for word in words] # [\"the\", \"short\", \"short\", \"words\"]"
  },
  {
    "objectID": "block_1/511_python/511_python.html#inline-if-else",
    "href": "block_1/511_python/511_python.html#inline-if-else",
    "title": "Python Basics",
    "section": "",
    "text": "# syntax: value_if_true if condition else value_if_false\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = \"long list\" if len(words) &gt; 10 else \"short list\""
  },
  {
    "objectID": "block_1/511_python/511_python.html#try-except",
    "href": "block_1/511_python/511_python.html#try-except",
    "title": "Python Basics",
    "section": "",
    "text": "try:\n        # code that might raise an exception\n    except:\n        # code to handle exception\n        raise TypeError(\"Error message\") # raise exception"
  },
  {
    "objectID": "block_1/511_python/511_python.html#functions",
    "href": "block_1/511_python/511_python.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "def function_name(arg1, arg2, arg3=default_value): # default values are optional\n        # code\n        return value\n\n    def function_name(*args): # takes in multiple arguments\n        for arg in args:\n            # code\n        return value\n\nside effects: If a function does anything other than returning a value, it is said to have side effects. An example of this is when a function changes the variables passed into it, or when a function prints something to the screen.\n\n\n\n    # Syntax: lambda arg1, arg2: expression\n    lambda x: x+1\n\n\n\n    # NumPy/SciPy style\n    def repeat_string(s, n=2):\n        \"\"\"\n        Repeat the string s, n times.\n\n        Parameters\n        ----------\n        s : str\n            the string\n        n : int, optional\n            the number of times, by default = 2\n\n        Returns\n        -------\n        str\n            the repeated string\n\n        Examples\n        --------\n        &gt;&gt;&gt; repeat_string(\"Blah\", 3)\n        \"BlahBlahBlah\"\n        \"\"\"\n        return s * n\n\n\n\n    def repeat_string(s: str, n: int = 2) -&gt; str:\n        return s * n"
  },
  {
    "objectID": "block_1/511_python/511_python.html#classes",
    "href": "block_1/511_python/511_python.html#classes",
    "title": "Python Basics",
    "section": "",
    "text": "class ClassName:\n        def __init__(self, arg1, arg2):\n            # code\n        def method_name(self, arg1, arg2):\n            # code\n        @classmethod\n        def other_method_name(cls, arg1, arg2):\n            # classmethod is used to create factory methods, aka other ways to create objects\n            # code\n\n    # Inheritance\n    class SubClassName(ClassName):\n        def __init__(self, arg1, arg2):\n            super().__init__(arg1, arg2)\n            # code\n        def method2_name(self, arg1, arg2):\n            # code\n\n\n    class ClassName:\n        static_var = 0\n\n        @staticmethod\n        def method_name(arg1, arg2):\n            # code\n            ClassName.static_var += 1\n            return ClassName.static_var\n        @staticmethod\n        def reset_static_var():\n            ClassName.static_var = 0\n\n\n\n    class TestVector(unittest.TestCase):\n        def test_str1(self):\n            v1 = Vector(1, 2, 3)\n            self.assertIn(\"Vector = [1, 2, 3]\", v1.__str__())\n            self.assertEqual(len(v1.elements), 3)\n\n        def test_str2(self):\n            v1 = Vector(500)\n            self.assertIn(\"Vector = [500]\", v1.__str__())\n\n\n    TestVector = unittest.main(argv=[\"\"], verbosity=0, exit=False)\n    assert TestVector.result.wasSuccessful()"
  },
  {
    "objectID": "block_1/511_python/511_python.html#pep8-guidelines",
    "href": "block_1/511_python/511_python.html#pep8-guidelines",
    "title": "Python Basics",
    "section": "",
    "text": "Category\nRule/Convention\nExample\n\n\n\n\nIndentation\nUse 4 spaces per indentation level\nif x: # four spaces here\n\n\n\nUse spaces around operators and after commas\na = b + c, d = e + f\n\n\nMaximum Line Length\nLimit all lines to a maximum of 79 characters for code, 72 for comments and docstrings\n\n\n\nImports\nAlways put imports at the top of the file\nimport os\n\n\n\nGroup imports: standard, third-party, local\nimport os import numpy as np from . import my_module\n\n\n\nUse absolute imports\nfrom my_pkg import module\n\n\nWhitespace\nAvoid extraneous whitespace\nspam(ham[1], {eggs: 2})\n\n\n\nUse blank lines to separate functions, classes, blocks of code inside functions\n\n\n\nComments\nComments should be complete sentences\n# This is a complete sentence.\n\n\n\nUse inline comments sparingly\nx = x + 1 # Increment x\n\n\nNaming Conventions\nFunction names should be lowercase with underscores\nmy_function()\n\n\n\nClass names should use CapWords convention\nMyClass\n\n\n\nConstants should be in all capital letters\nCONSTANT_NAME\n\n\nString Quotes\nUse double quotes for docstrings and single quotes for everything else when you start a project (or adhere to the project’s conventions)\n‘string’, “““docstring”“”\n\n\nExpressions and Statements\nDon’t use semicolons to terminate statements\nx = 1 (not x = 1;)\n\n\n\nUse is for identity comparisons and == for value comparisons\nif x is None\n\n\nOther Recommendations\nUse built-in types like list, dict instead of List, Dict from the typing module for simple use-cases\ndef func(a: list) -&gt; None:"
  },
  {
    "objectID": "block_1/511_python/511_python.html#numpy-arrays",
    "href": "block_1/511_python/511_python.html#numpy-arrays",
    "title": "Python Basics",
    "section": "Numpy Arrays",
    "text": "Numpy Arrays\n\nDifference with lists:\n\n\n\n\n\n\n\n\nFeature\nNumpy arrays\nLists\n\n\n\n\nData type uniformity\nAll elements must be of the same data type\nNo restriction\n\n\nStorage efficiency\nStored more efficiently\nLess efficient\n\n\nVectorized operations\nSupports\nDoes not support\n\n\n\n\n\nCreating arrays:\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\nnp.array([list], dtype)\nCreate an array from a list\nnp.array([1, 2, 3], dtype='int8')\n\n\nnp.arange(start, stop, step)\nCreate an array of evenly spaced values\nnp.arange(0, 10, 2) returns [0, 2, 4, 6, 8]\n\n\nnp.ones(shape, dtype)\nCreate an array of ones\nnp.ones((3, 2), dtype='int8') returns [[1, 1], [1, 1], [1, 1]]\n\n\nnp.zeros(shape, dtype)\nCreate an array of zeros\n\n\n\nnp.full(shape, fill_value, dtype)\nCreate an array of a specific value\n\n\n\nnp.random.rand(shape)\nCreate an array of random values between 0 and 1\nnp.random.randn(3, 3, 3)\n\n\n\n\n\nOther useful functions:\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\narray.transpose() or array.T\nTranspose an array\n\n\n\narray.ndim\nReturns the number of dimensions\nx = np.ones((3,2)) and print(x.ndim) returns 2\n\n\narray.size\nReturns the number of elements\nprint(x.size) returns 6\n\n\narray.shape\nReturns the shape of the array\nprint(x.shape) returns (3, 2)\n\n\nnp.flip(array)\nReverse an array , default: flips row and cols\nnp.flip(array)\n\n\narray.reshape(shape)\nReshape an array\narray.reshape(2, 3) for 2 rows, 3 columns\n\n\narray.sort()\nSort an array in ascending order\nnew_arr = array.sort()\n\n\narray.flatten()\nFlatten an array, same as reshape(-1)\nnew_arr = array.flatten()\n\n\narray.concatenate()\nConcatenate arrays, default: axis=0 (adds to the bottom)\nnp.concatenate([array1, array2], axis=0)\n\n\n\nNote: For the reshape function, you can use -1 to infer dimension (e.g. array.reshape(-1, 3)). Also, the default is row-major order, but you can specify order='F' for column-major order.\n\n\narray.dtype: returns the data type\narray.astype(dtype): convert an array to a different data type\nnp.array_equal(array1, array2): check if two arrays are equal\n\n\n\nArray operations and broadcasting\n\nsmaller arrays are broadcasted to match the shape of larger arrays \nCan only broadcast if compatible in all dimensions. They are compatible if:\n\nthey are equal in size\none of them is 1\n\nChecks starting from the right-most dimension"
  },
  {
    "objectID": "block_1/511_python/511_python.html#series",
    "href": "block_1/511_python/511_python.html#series",
    "title": "Python Basics",
    "section": "Series",
    "text": "Series\n\nnumpy array with labels\ncan store any data type, string takes the most space\n\nIf any NaN, dtype of Series becomes float64\nif any mixed data types, dtype of Series becomes object\n\npd.Series(): create a Series\n\npd.Series([1, 2, 3], index=['a', 'b', 'c'], name='col1'): create a Series from a list with labels and a name\npd.Series({'a': 1, 'b': 2, 'c': 3}): create a Series from a dictionary\n\ns.index: returns the index\ns.to_numpy(): convert a Series to a numpy array\n\n\nIndexing:\n\ns['a']: returns the value at index ‘a’\ns[['a', 'b']]: returns a Series with values at indices ‘a’ and ‘b’\ns[0:3]: returns a Series with values at indices 0, 1, and 2\n\n\n\nOperations:\n\naligns values based on index\ns1 + s2: returns a Series with values at indices in both s1 and s2\n\nwill return NaN (Not a Number) if index is not in both s1 and s2\nkind of like a left join in SQL"
  },
  {
    "objectID": "block_1/511_python/511_python.html#dataframes",
    "href": "block_1/511_python/511_python.html#dataframes",
    "title": "Python Basics",
    "section": "DataFrames",
    "text": "DataFrames\n\nCreating DataFrames\n\npd.DataFrame([2d list], columns=['col1', 'col2'], index=['row1', 'row2']): creates a DataFrame from a list with column names and row names\n\n\n\n\n\n\n\n\nSource\nCode\n\n\n\n\nLists of lists\npd.DataFrame([['Quan', 7], ['Mike', 15], ['Tiffany', 3]])\n\n\nndarray\npd.DataFrame(np.array([['Quan', 7], ['Mike', 15], ['Tiffany', 3]]))\n\n\nDictionary\npd.DataFrame({\"Name\": ['Quan', 'Mike', 'Tiffany'], \"Number\": [7, 15, 3]})\n\n\nList of tuples\npd.DataFrame(zip(['Quan', 'Mike', 'Tiffany'], [7, 15, 3]))\n\n\nSeries\npd.DataFrame({\"Name\": pd.Series(['Quan', 'Mike', 'Tiffany']), \"Number\": pd.Series([7, 15, 3])})\n\n\nCsv\npd.read_csv('file.csv', sep='\\t')\n\n\n\n\n\nIndexing\n\n\n\n\n\n\n\n\nMethod\nDescription\nExample\n\n\n\n\nSingle Column\nReturns single column as a Series\ndf['col1'] or df.col1\n\n\n\nReturns single column as a DataFrame\ndf[['col1']]\n\n\nMultiple Columns\nReturns multiple columns as a DataFrame\ndf[['col1', 'col2']]\n\n\niloc (integer)\nReturns first row as a Series\ndf.iloc[0]\n\n\n\nReturns first row as a DataFrame\ndf.iloc[0:1] OR df.iloc[[0]]\n\n\n\nReturns specific rows and columns\ndf.iloc[2:5, 1:4] (rows 2-4 and columns 1-3)\n\n\n\nReturns specific rows and columns\ndf.iloc[[0, 2], [0, 2]] (rows 0 and 2 and columns 0 and 2)\n\n\nloc (label)\nReturns all rows and a specific column as a Series\ndf.loc[:, 'col1']\n\n\n\nReturns all rows and a specific column as a DataFrame\ndf.loc[:, ['col1']]\n\n\n\nReturns all rows and specific range of columns as a DataFrame\ndf.loc[:, 'col1':'col3']\n\n\n\nReturns item at row1, col1\ndf.loc['row1', 'col1']\n\n\nBoolean indexing\nReturns rows based on a boolean condition\ndf[df['col1'] &gt; 5]\n\n\nquery\nReturns rows based on a query (same as above)\ndf.query('col1 &gt; 5')\n\n\n\nNote: Indexing with just a single number like df[0] or a slice like df[0:1] without iloc or loc doesn’t work for DataFrame columns.\n\nCan do indexing with boolean with loc: df.loc[:, df['col1'] &gt; 5] Gets all rows and columns where col1 &gt; 5\nWant to *2 for rows in col2 when col1&gt;5: df.loc[df['col1'] &gt; 5, 'col2'] = 0\n\n\n\nOther useful functions\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\nmax()\nReturns the maximum value\ndf['col1'].max()\n\n\nidxmax()\nReturns the index of the maximum value\ndf['col1'].idxmax()\n\n\nmin()\nReturns the minimum value\ndf['col1'].min()\n\n\nidxmin()\nReturns the index of the minimum value\ndf['col1'].idxmin()\n\n\n\nCan return row of max value of col1 by: df.iloc[[df['col1'].idxmax()]] or df.iloc[df.loc[:, 'col1'].idxmax()]"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-wrangling",
    "href": "block_1/511_python/511_python.html#data-wrangling",
    "title": "Python Basics",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData Summary\n\nshape: returns (rows, columns)\ninfo(): returns column names, data types, and number of non-null values\ndescribe(): returns summary statistics for each column\n\n\n\nViews vs Copies\n\nCorrect way to replace: df.loc[df['Released_Year'] &gt; 2021, 'Released_Year'] = 2010"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-manipulation",
    "href": "block_1/511_python/511_python.html#data-manipulation",
    "title": "Python Basics",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nrename(): rename columns\n\ndf.rename(columns={'old_name': 'new_name', 'old_name_2': 'new_name_2'}, inplace=True)\ninplace=True to modify DataFrame, instead of returning a new DataFrame, default is False\nrecommended to just assign to a new variable\n\ncolumns: returns column names, can change column names by assigning a list of new names\n\ndf.columns.to_list(): returns column names as list\n\n\n\nChanging Index\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ndf.set_index('col1', inplace=True)\nSet a column as the index\n\n\ndf = pd.read_csv('file.csv', index_col='col1')\nSet index when reading in a file\n\n\ndf.reset_index()\nReset index to default, starting at 0\n\n\ndf.index = df['col1']\nDirectly modify index\n\n\ndf.index.name = \"a\"\nRename index to ‘a’\n\n\n\n\n\nAdding/ Removing Columns\n\ndrop(): remove columns\n\ndf.drop(['col1', 'col2'], axis=1, inplace=True)\ndf.drop(df.columns[5:], axis=1)\n\ninsert(): insert a column at a specific location\n\ndf.insert(0, 'col1', df['col2'])\n\ndf['new_col'] = df['col1'] + df['col2']: add a new column\n\n\n\nAdding/ Removing Rows\n\ndf.drop([5:], axis=0): remove rows 5 and after\ndf = df.iloc[5:]: returns rows 5 and after\n\n\n\nReshaping Data\n\nmelt(): unpivot a DataFrame from wide to long format\n\ne.g: df_melt = df.melt(id_vars=\"Name\", value_vars=[\"2020\", \"2019\"], var_name=\"Year\", value_name=\"Num_Courses\")\nid_vars: specifies which column should be used as identifier variables (the key)\nvalue_vars: Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.\nvar_name: select which variables to melt\nvalue_name: select which variables to keep\nignore_index=False: keeps the index, default is True (resets index)\n\npivot(): pivot a DataFrame from long to wide format\n\ne.g: df_pivot = df_melt.pivot(index=\"Name\", columns=\"Year\", values=\"Num_Courses\")\nindex: specifies which column should be used as index\ncolumns: specifies which column should be used as columns\nvalues: specifies which column should be used as values\n\n\nAfter pivot, if want to remove column names, use df.columns.name = None\n\npivot_table(): pivot a DataFrame from long to wide format, but can handle duplicate values\n\ne.g: df_pivot = df.pivot_table(index=\"Name\", columns=\"Year\", values=\"Num_Courses\", aggfunc='sum')\naggfunc: specifies how to aggregate duplicate values\n\nconcat(): concatenate DataFrames\n\ndf = pd.concat([df1, df2], axis=0, ignore_index=True)\naxis=0: concatenate along rows\naxis=1: concatenate along columns\nignore_index=True: ignore index (resets the index), default is False (leaves index as is)\n\nmerge(df1, df2, on='col1', how='inner'): merge DataFrames, analogous to join in R\n\non: specifies which column to merge on\nhow: specifies how to merge\n\ninner: only keep rows that are in both DataFrames\nouter: keep all rows from both DataFrames\nleft: keep all rows from the left DataFrame\nright: keep all rows from the right DataFrame\n\n\n\n\n\nApply Functions\n\ndf.apply(): apply a function to a DataFrame column or row-wise, takes/returns series or df\n\ndf.apply(lambda x: x['col1'] + x['col2'], axis=1): apply a function to each row\ndf.apply(lambda x: x['col1'] + x['col2'], axis=0): apply a function to each column\ndf['col1'].apply(lambda x: x + 1): apply a function to column ‘col1’\n\ndf.applymap(): apply a function to each element in a DataFrame\n\ndf.loc[:, [\"Mean Max Temp (°C)\"]].applymap(int): apply int to each element in column ’Mean Max Temp (°C)\ndf.loc[[\"Mean Max Temp (°C)\"]].astype(int): Faster…\nOnly works for DataFrames, not Series\n\n\n\n\nGrouping Data\n\ngroupby(): group data by a column\n\ndf.groupby('col1'): returns a DataFrameGroupBy object\ndf.groupby('col1').groups: returns a dictionary of groups\ndf.groupby('col1').get_group('group1'): returns a DataFrame of group1\n\ndf.agg(): aggregate data\n\ndf.groupby('col1').agg({'col2': 'mean', 'col3': 'sum'}): returns a DataFrame with mean of col2 and sum of col3 for each group (needs to be numeric, or else error)"
  },
  {
    "objectID": "block_1/511_python/511_python.html#string-dtype",
    "href": "block_1/511_python/511_python.html#string-dtype",
    "title": "Python Basics",
    "section": "String dtype",
    "text": "String dtype\n\ndf.str.func(): can apply any string function to each string in df\n\ne.g. df['col1'].str.lower(): convert all strings in col1 to lowercase\ndf['col1'].str.cat(sep=' '): concatenate all strings in col1 with a space in between, default is no space\n\nCan do regex as well:\n\ndf['col1'].str.contains(r'regex'): returns a boolean Series\ndf['col1'].str.replace(r'regex', 'new_string'): replace all strings in col1 that match regex with ‘new_string’"
  },
  {
    "objectID": "block_1/511_python/511_python.html#datetime-dtype",
    "href": "block_1/511_python/511_python.html#datetime-dtype",
    "title": "Python Basics",
    "section": "Datetime dtype",
    "text": "Datetime dtype\nfrom datetime import datetime, timedelta\n\nConstruct a datetime object: datetime(year, month, day, hour, minute, second)\ndatetime.now(): returns current datetime\ndatetime.strptime('July 9 2005, 13:54', '%B %d %Y, %H:%M'): convert a string to a datetime object\nAdd time to a datetime object: datetime + timedelta(days=1, hours=2, minutes=3, seconds=4)\n\n\nDatetime dtype in Pandas\n\npd.Timestamp('2021-07-09 13:54'): convert a string to a datetime object\n\npd.Timestamp(year, month, day, hour, minute, second): construct a datetime object\npd.Timestamp(datetime(year, month, day, hour, minute, second)): convert a datetime object to a Timestamp object\n\npd.Period('2021-07'): convert a string to a Period object\n\npd.Period(year, month): construct a Period object\npd.Period(datetime(year, month, day, hour, minute, second)): convert a datetime object to a Period objects\n\n\nConverting existing columns to datetime dtype:\n\npd.to_datetime(df['col1']): convert col1 to datetime dtype\npd.read_csv('file.csv', parse_dates=True): convert all columns with datetime dtype to datetime dtype\n\n\n\nOther datetime functions\nThe index of a DataFrame can be a datetime dtype. These functions can be applied to the index.\n\ndf.index.year: returns a Series of years\ndf.index.month: returns a Series of months\n\ndf.index.month_name(): returns a Series of month names\n\ndf.between_time('9:00', '12:00'): returns rows between 9am and 12pm\ndf.resample('D').mean(): resample data to daily and take the mean\n\nD: daily\nW: weekly\nM: monthly\nQ: quarterly\nY: yearly\n\npd.date_range(start, end, freq): returns a DatetimeIndex\n\nfreq same as above (e.g. D, W, M, Q, Y)"
  },
  {
    "objectID": "block_1/511_python/511_python.html#visualization",
    "href": "block_1/511_python/511_python.html#visualization",
    "title": "Python Basics",
    "section": "Visualization",
    "text": "Visualization\n\ndf['Distance'].plot.line(): plot a line chart\ndf['Distance'].plot.bar(): plot a bar chart\ndf['Distance'].cumsum().plot.line(): plot a line chart of cumulative sum"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html",
    "href": "block_1/521_platforms/521_platforms.html",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "ls - list files and directories. Default behavior is to list the contents of the current working directory.\ncd - change directory. Used to navigate the filesystem. Default behavior is to change to the home directory.\npwd - print working directory. It will return the absolute path of the current working directory.\nmkdir - make directory\ntouch - create file\nrm - remove file\nrmdir - remove directory\nmv - move file. also used to rename file.\ncp - copy file\nwhich - locate a program. It will return the path of the program.\n\n\n\nThese flags allow us to modify the default behaviour of a program.\n\n-a - all\n-l - long\n-h - human readable\n-r - recursive\n-f - force\n\n\n\n\n\n. - current directory\n.. - parent directory\n~ - home directory\n/ - root directory\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGraphical user interface (GUI)\nA user interface that relies on windows, menus, pointers, and other graphical elements\n\n\nCommand-line interface (CLI)\nA user interface that relies solely on text for commands and output, typically running in a shell.\n\n\nOperating system\nA program that provides a standard interface to whatever hardware it is running on.\n\n\nFilesystem\nThe part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored.\n\n\nSubdirectory\nA directory that is below another directory\n\n\nParent directory\nThe directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem.\n\n\nHome directory\nA directory that contains a user’s files.\n\n\nCurrent working directory\nThe folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory.\n\n\nPath (in filesystem)\nA string that specifies a location in a filesystem.\n\n\nAbsolute path\nA path that points to the same location in the filesystem regardless of where it is evaluated. It is the equivalent of latitude and longitude in geography.\n\n\nRelative path\nA path whose destination is interpreted relative to some other location, such as the current working directory.\n\n\nDirectory\nAn item within a filesystem that can contain files and other directories. Also known as “folder”.\n\n\nRoot directory\nThe directory that contains everything else, either directly or indirectly.\n\n\nPrompt\nThe text printed by the shell that indicates it is ready to accept another command.\n\n\n\n\n\n\n\nSymbol\n\n\n\n\nA. Root directory (see note)\n/\n\n\nB. Parent directory\n..\n\n\nC. Current working directory\n.\n\n\nD. Home directory\n~ \n\n\nE. Command line argument\n-y or --yes\n\n\nF. Prompt (R)\n&gt;\n\n\nG. Prompt (Python)\n&gt;&gt;&gt;\n\n\nH. Prompt (Bash)\n$\n\n\n\n\n\n\n\n\ngit init - initialize a git repository\ngit add - add files to staging area.\n\nStaging area is a place where we can group files together before we “commit” them to git.\n\ngit commit - commit changes to git. Records a new version of the files in the repository.\ngit push - push changes to remote repository from local repository\ngit pull - pull changes from remote repository to local repository\ngit status - check status of git repository\ngit log - check commit history\n\ngit log --oneline - check commit history in one line\ngit log -p - check commit history with changes\n\ngit diff - check difference between two commits\ngit reset - reset git repository\n\ngit reset --hard - reset git repository to last commit\ngit reset --soft - reset git repository to last commit but keep changes\n\ngit revert - revert git repository. The difference between revert and reset is that revert creates a new commit with the changes from the commit we want to revert.\ngit stash - saves changes that you don’t want to commit immediately. It takes the dirty state of your working directory — that is, your modified tracked files and staged changes — and saves it on a stack of unfinished changes that you can reapply at any time.\n\n\n\n\nRepository - a collection of files and folders that are tracked by git.\nCommit - a snapshot of the repository at a specific point in time.\nCommit Hash - These are the commits’ hashes (SHA-1), which are used to uniquely identify the commits within a project.\nBranch - a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the “live” version.\n\n\n\n\n\nPublic key is used to encrypt data and private key is used to decrypt data.\nConfidentiality - only the intended recipient can decrypt the data.\nAuthentication - only the intended recipient can encrypt the data.\n\n\n\n\nSSH is more secure than HTTPS because it uses public and private keys to encrypt and decrypt data. HTTPS uses username and password to authenticate users.\n\n\n\n\nGithub pages will look in either the repository root / directory or in the repository docs/ directory for website content to render.\n\n\n\n\nDynamic Documents: Rooted in Knuth’s “literate programming” concept from 1984.\nMain Goals:\n\nWrite program code.\nCreate narratives to elucidate code function.\n\nBenefits:\n\nEnhances understanding and provides comprehensive documentation.\nGives a way to run code and view results.\nAllows text and code to be combined in a single document.\nFacilitates reproducibility of results and diagrams.\n\nPopular Formats:\n\nJupyter Notebooks (.ipynb)\nRMarkdown documents (.Rmd)\n\nKey Features:\n\nNarratives formatted with markdown.\nExecutable code:\n\nInterwoven in text (RMarkdown’s inline code).\nSeparate sections: code cells (Jupyter) or code chunks (RMarkdown)."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#shell",
    "href": "block_1/521_platforms/521_platforms.html#shell",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "ls - list files and directories. Default behavior is to list the contents of the current working directory.\ncd - change directory. Used to navigate the filesystem. Default behavior is to change to the home directory.\npwd - print working directory. It will return the absolute path of the current working directory.\nmkdir - make directory\ntouch - create file\nrm - remove file\nrmdir - remove directory\nmv - move file. also used to rename file.\ncp - copy file\nwhich - locate a program. It will return the path of the program.\n\n\n\nThese flags allow us to modify the default behaviour of a program.\n\n-a - all\n-l - long\n-h - human readable\n-r - recursive\n-f - force\n\n\n\n\n\n. - current directory\n.. - parent directory\n~ - home directory\n/ - root directory\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGraphical user interface (GUI)\nA user interface that relies on windows, menus, pointers, and other graphical elements\n\n\nCommand-line interface (CLI)\nA user interface that relies solely on text for commands and output, typically running in a shell.\n\n\nOperating system\nA program that provides a standard interface to whatever hardware it is running on.\n\n\nFilesystem\nThe part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored.\n\n\nSubdirectory\nA directory that is below another directory\n\n\nParent directory\nThe directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem.\n\n\nHome directory\nA directory that contains a user’s files.\n\n\nCurrent working directory\nThe folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory.\n\n\nPath (in filesystem)\nA string that specifies a location in a filesystem.\n\n\nAbsolute path\nA path that points to the same location in the filesystem regardless of where it is evaluated. It is the equivalent of latitude and longitude in geography.\n\n\nRelative path\nA path whose destination is interpreted relative to some other location, such as the current working directory.\n\n\nDirectory\nAn item within a filesystem that can contain files and other directories. Also known as “folder”.\n\n\nRoot directory\nThe directory that contains everything else, either directly or indirectly.\n\n\nPrompt\nThe text printed by the shell that indicates it is ready to accept another command.\n\n\n\n\n\n\n\nSymbol\n\n\n\n\nA. Root directory (see note)\n/\n\n\nB. Parent directory\n..\n\n\nC. Current working directory\n.\n\n\nD. Home directory\n~ \n\n\nE. Command line argument\n-y or --yes\n\n\nF. Prompt (R)\n&gt;\n\n\nG. Prompt (Python)\n&gt;&gt;&gt;\n\n\nH. Prompt (Bash)\n$"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#git-and-github",
    "href": "block_1/521_platforms/521_platforms.html#git-and-github",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "git init - initialize a git repository\ngit add - add files to staging area.\n\nStaging area is a place where we can group files together before we “commit” them to git.\n\ngit commit - commit changes to git. Records a new version of the files in the repository.\ngit push - push changes to remote repository from local repository\ngit pull - pull changes from remote repository to local repository\ngit status - check status of git repository\ngit log - check commit history\n\ngit log --oneline - check commit history in one line\ngit log -p - check commit history with changes\n\ngit diff - check difference between two commits\ngit reset - reset git repository\n\ngit reset --hard - reset git repository to last commit\ngit reset --soft - reset git repository to last commit but keep changes\n\ngit revert - revert git repository. The difference between revert and reset is that revert creates a new commit with the changes from the commit we want to revert.\ngit stash - saves changes that you don’t want to commit immediately. It takes the dirty state of your working directory — that is, your modified tracked files and staged changes — and saves it on a stack of unfinished changes that you can reapply at any time.\n\n\n\n\nRepository - a collection of files and folders that are tracked by git.\nCommit - a snapshot of the repository at a specific point in time.\nCommit Hash - These are the commits’ hashes (SHA-1), which are used to uniquely identify the commits within a project.\nBranch - a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the “live” version.\n\n\n\n\n\nPublic key is used to encrypt data and private key is used to decrypt data.\nConfidentiality - only the intended recipient can decrypt the data.\nAuthentication - only the intended recipient can encrypt the data.\n\n\n\n\nSSH is more secure than HTTPS because it uses public and private keys to encrypt and decrypt data. HTTPS uses username and password to authenticate users."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#quarto-and-github-pages",
    "href": "block_1/521_platforms/521_platforms.html#quarto-and-github-pages",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Github pages will look in either the repository root / directory or in the repository docs/ directory for website content to render."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#dynamic-documents",
    "href": "block_1/521_platforms/521_platforms.html#dynamic-documents",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Dynamic Documents: Rooted in Knuth’s “literate programming” concept from 1984.\nMain Goals:\n\nWrite program code.\nCreate narratives to elucidate code function.\n\nBenefits:\n\nEnhances understanding and provides comprehensive documentation.\nGives a way to run code and view results.\nAllows text and code to be combined in a single document.\nFacilitates reproducibility of results and diagrams.\n\nPopular Formats:\n\nJupyter Notebooks (.ipynb)\nRMarkdown documents (.Rmd)\n\nKey Features:\n\nNarratives formatted with markdown.\nExecutable code:\n\nInterwoven in text (RMarkdown’s inline code).\nSeparate sections: code cells (Jupyter) or code chunks (RMarkdown)."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#rstudio-basics",
    "href": "block_1/521_platforms/521_platforms.html#rstudio-basics",
    "title": "Platforms Cheat Sheet",
    "section": "RStudio Basics",
    "text": "RStudio Basics\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ngetwd()\nReturns the current working directory of the R session.\n\n\nsetwd(path)\nChanges the working directory to the specified path.\n\n\nhere::here()\nCreates file paths relative to the project’s root (where the .Rproj file is) to ensure consistent and portable references.\n\n\n\nNote: For portability, prefer here::here() over setwd() to avoid path inconsistencies across different systems.\n\nRStudio code chunks\n\nGlobal looks like this:"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#quarto",
    "href": "block_1/521_platforms/521_platforms.html#quarto",
    "title": "Platforms Cheat Sheet",
    "section": "Quarto",
    "text": "Quarto\n\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nJupyter Notebooks\nRMarkdown\nQuarto (.qmd)\n\n\n\n\nPrimary Language\nPython\nR\nPython, R, Julia\n\n\nSecondary Language\nR (via R kernel)\nPython (via reticulate)\nR with Python (via reticulate), Python with R (via rpy2)\n\n\nCompatible Editors\nJupyterLab, VS Code\nRStudio\nRStudio, VS Code, JupyterLab\n\n\nSpecial Features\n-\n-\nSingle engine processing, cross-language integration\n\n\nRecommended Environments\nJupyterLab or VS Code\nRStudio\nRStudio (offers code-completion, incremental cell execution, and other tools for working with executable code)\n\n\n\n\nRMarkdown Templates\n\nOn top of the RMarkdown document, you can specify a template.\nTemplates are .Rmd files that contain the YAML header and some default text.\n\n---\ntitle: \"Untitled\"\noutput: github_document / html_document / pdf_document / word_document\nauthor: \"farrandi\"\ndate: \"10/2/2023\"\n---\n\n\nBasic Quarto presentation:\n# Same YAML header as above\n---\ntitle: \"521 Quiz 2\"\nauthor: \"Daniel Chen\"\nformat: revealjs\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#presentations-in-jupyterlab",
    "href": "block_1/521_platforms/521_platforms.html#presentations-in-jupyterlab",
    "title": "Platforms Cheat Sheet",
    "section": "Presentations in JupyterLab",
    "text": "Presentations in JupyterLab\n\nJupyterLab supports presentations using the JavaScript framework: reveal.js.\nreveal.js is the same framework used by Quarto.\nCells for presentations are marked via the property inspector:\n\nFound at the settings wheel in the left side panel.\nSelect slide type from the dropdown menu.\n\nreveal.js presentations are two dimensional:\n\nHorizontal slides\nVertical sub-slides\n\n\n\nSlide Types and Navigation\n\n\n\n\n\n\n\n\nSlide Type\nDescription\nNavigation\n\n\n\n\nSlide\nStandard slide\nLeft and right arrows\n\n\nSub-slide\nSub-topic of a slide\nUp and down arrows\n\n\nFragment\nAnimated part of the previous slide, e.g. a bullet\nPart of slide animation\n\n\n- (or empty)\nAppears in same cell as previous slide\nPart of slide\n\n\nNotes\nSpeaker notes\nVisible when pressing ‘t’\n\n\nSkip\nNot included in the presentation\n-\n\n\n\n\n\nImage Inclusion Workaround\n\nImages using ![]() or &lt;img&gt; tags don’t show up in exports.\nWorkaround: Paste images into a Markdown cell to include them as attachments. This ensures visibility in HTML and slide exports."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#virtual-environments",
    "href": "block_1/521_platforms/521_platforms.html#virtual-environments",
    "title": "Platforms Cheat Sheet",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual Environments: Isolated Python environments that allow for the installation of packages without affecting the system’s Python installation.\nBenefits:\n\nIsolation: Packages installed in a virtual environment are isolated from the system’s Python installation.\nReproducibility: Virtual environments can be shared with others to ensure reproducibility.\nVersion control: Allow managing the versions of libraries and tools used in a project.\nCross-platform: Ensures that the project’s dependencies are consistent across different systems.\nExperimentation: Allows for experimentation with different versions of packages.\nClean environment: When starting a new project, starts with a clean slate.\nConsistency: Ensures that the project’s dependencies are consistent across different systems.\nSecurity: Isolates the project’s dependencies from the system’s Python installation.\n\n\n\nPython: Conda\n\nconda create -n &lt;env_name&gt; python=&lt;version&gt;: create a new environment\n\nconda env create -f path/to/environment.yml: create an environment from a file\nconda create --name live_env --clone test_env: create an environment from an existing environment\n\nconda env list: list all environments\nconda activate &lt;env_name&gt;: activate the environment\nconda deactivate: deactivate the environment\nconda remove -n &lt;env_name&gt; --all: remove the environment\nconda env export -f environment.yml: export the environment to a file\n\nManaging packages:\n\nconda config --add channels conda-forge: add a channel\nconda list: list all packages in the environment\nconda search &lt;package&gt;: search for a package\nconda install &lt;package&gt;: install a package\n\nconda install &lt;package&gt;=&lt;version&gt;: install a specific version of a package\n\nconda remove &lt;package&gt;: remove a package\n\nExample environment.yml file:\nname: test_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - conda\n  - python=3.7\n  - pandas==1.0.2\n  - jupyterlab\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nname\n- Identifies the environment’s name. - Useful for distinguishing multiple environments.\n\n\nchannels\n- Locations where Conda searches for packages. - Default: defaults channel. - Popular options: conda-forge, bioconda.\n\n\ndependencies\n- Lists required packages for the environment. - Can specify versions or ranges. - Can include Conda or pip packages.\n\n\nprefix\n- (Optional) Directory where Conda installs the environment. - Defaults to Conda’s main directory if not provided.\n\n\n\n\n\nR: renv\n\nMake a new project in RStudio 1.1. Select setting to use renv 1.2. library(renv): to use renv\nrenv::init(): initialize the project"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#file-names",
    "href": "block_1/521_platforms/521_platforms.html#file-names",
    "title": "Platforms Cheat Sheet",
    "section": "File names",
    "text": "File names\n3 Principles:\n\nMachine readable:\n\nregex and globbing friendly (avoid spaces, special characters, case sensitivity, etc.)\ndeliberate use of delimiters (e.g. _, -, .)\n\nHuman readable: Helps other people and ourselves in the future quickly understand the file structure and contents of a project/ file.\nPlays well with default ordering: Makes files more organized and easily searchable. Easy for us humans to find the files we are looking for.\n\nDates go: YYYY-MM-DD"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#asking-questions",
    "href": "block_1/521_platforms/521_platforms.html#asking-questions",
    "title": "Platforms Cheat Sheet",
    "section": "Asking Questions",
    "text": "Asking Questions\n\nReproducible example\nCode formatting\nMinimal, complete, verifiable example\n\nEffective Questioning & Creating an MRE:\n\nSearch for similar questions before asking.\nClearly state the problem in the title and provide brief details in the body.\nProvide the shortest version of your code that replicates the error.\nInclude definitions if you’ve used functions or classes.\nUse toy datasets rather than real data.\nUse markdown for code to ensure readability and syntax highlighting.\nShare attempts, points of confusion, and full error tracebacks."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#regex",
    "href": "block_1/521_platforms/521_platforms.html#regex",
    "title": "Platforms Cheat Sheet",
    "section": "Regex",
    "text": "Regex"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Made by @farrandi\nThis is a list of cheatsheets from my classes in the Master of Data Science program at the University of British Columbia."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html",
    "href": "block_2/571_sup_learn/571_sup_learn.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Machine Learning: A field of study that gives computers the ability to learn without being explicitly programmed.\n\nauto detect patterns in data and make predictions\nPopular def of supervised learning: input: data + labels, output: model\n\nTraining a model from input data and its corresponding targets to predict targets for new examples.\n\n\nFundamental goal of machine learning: generalize beyond the examples in the training set.\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nThe training data includes the desired solutions, called labels.\nThe training data is unlabeled.\n\n\nSubtypes/Goals\n- Predict based on input data and its corresponding targets- Classification: Predict a class label e.g. will someone pass or fail final based on previous scores- Regression: Predict a continuous number e.g. what score will someone get on final based on previous score\n- Clustering: Group similar instances- Anomaly detection: Detect abnormal instances- Visualization and dimensionality reduction: Simplify data- Association rule learning: Discover relations between attributes\n\n\nOther Examples\nDecision tree, naive bayes, kNN, random forest, support vector machine, neural network\nk-means, hierarchical cluster analysis, expectation maximization, t-SNE, apriori, FP-growth\n\n\n\n\n\n\n\nFeatures/ X[n x d]: The input variables used to make predictions. (d = # of features)\nTarget/ y [n x 1]: The output variable we are trying to predict.\nExample: A particular instance of data, usually represented as a vector of features. (n = # of training examples)\nTraining: Fitting the model to examples.\nLabel: The target value for a particular example. (y)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nDescription\nThe coefficients learned by the model during training.\nSettings used to control the training process.\n\n\nLearning/Setting Process\nAre learned automatically during training.\nAre set before training.\n\n\nPurpose or Role\nRule values that are learned from the training data (examples/features).\nControls how complex the model is. Validate using validation score (can overfit if too complex).\n\n\nExamples\ne.g. coefficients in linear regression, weights in neural networks\ne.g. learning rate, number of iterations, number of hidden layers, depth of decision tree, k in kNN and k-means\n\n\n\n\n\n\n\n\nGenerally there are 2 kinds of error:\n\nTraining error (\\(E_{train}\\)): Error on the training data.\nDistribution error/ test error/ generalization error (\\(E_{D}\\)): Error on new data.\n\n\\(E\\_{approx} = E_{D} - E_{train}\\)\n\n\n\n\n\nTraining set: The data used to train the model. Used a lot to set parameters.\nValidation set: The data used to evaluate the model during training. Used a few times to set hyperparameters.\nTest set: The data used to evaluate the model after training. Used once to estimate \\(E_{D}\\).\nDeployment: The model is used in the real world.\n\nfrom sklearn.model_selection import train_test_split\n\n# Used directly on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)  # 80%-20% train test split on X and y\n\n# Used on a dataframe\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, random_state=123\n)  # 80%-20% train test split on df\n\n\n\nA method of estimating \\(E_{D}\\) using the training set.\n\nDivide the training set into \\(k\\) folds.\nFor each fold, train on the other \\(k-1\\) folds and evaluate on the current fold.\n\nBenefits:\n\nMore accurate estimate of \\(E_{D}\\). Sometimes just unlucky with train/test split, this helps.\nMore efficient use of data.\n\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# cross_val_score not as comprehensive as cross_validate\n# cross_val_score only returns the scores\ncv_scores = cross_val_score(model, X_train, y_train, cv=10)\n\n# using cross_validate\nscores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n# returns a dictionary with keys: ['fit_time', 'score_time', 'test_score', 'train_score']\npd.DataFrame(scores)\ncross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to evaluate how well the model will generalize to unseen data.\n\n\n\n\n\nAKA the bias/ variance trade-off in supervised learning.\n\nBias: Tendency to consistently learn the same wrong thing (high bias = underfitting).\nVariance: Tenency to learn random things irrespective of the real signal (high variance = overfitting).\n\nAs you increase model complexity, \\(E_{train}\\) goes down but \\(E_{approx} = E_{D} - E_{train}\\) goes up."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#supervised-vs.-unsupervised-learning",
    "href": "block_2/571_sup_learn/571_sup_learn.html#supervised-vs.-unsupervised-learning",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nThe training data includes the desired solutions, called labels.\nThe training data is unlabeled.\n\n\nSubtypes/Goals\n- Predict based on input data and its corresponding targets- Classification: Predict a class label e.g. will someone pass or fail final based on previous scores- Regression: Predict a continuous number e.g. what score will someone get on final based on previous score\n- Clustering: Group similar instances- Anomaly detection: Detect abnormal instances- Visualization and dimensionality reduction: Simplify data- Association rule learning: Discover relations between attributes\n\n\nOther Examples\nDecision tree, naive bayes, kNN, random forest, support vector machine, neural network\nk-means, hierarchical cluster analysis, expectation maximization, t-SNE, apriori, FP-growth"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#common-terms",
    "href": "block_2/571_sup_learn/571_sup_learn.html#common-terms",
    "title": "Supervised Learning",
    "section": "",
    "text": "Features/ X[n x d]: The input variables used to make predictions. (d = # of features)\nTarget/ y [n x 1]: The output variable we are trying to predict.\nExample: A particular instance of data, usually represented as a vector of features. (n = # of training examples)\nTraining: Fitting the model to examples.\nLabel: The target value for a particular example. (y)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nDescription\nThe coefficients learned by the model during training.\nSettings used to control the training process.\n\n\nLearning/Setting Process\nAre learned automatically during training.\nAre set before training.\n\n\nPurpose or Role\nRule values that are learned from the training data (examples/features).\nControls how complex the model is. Validate using validation score (can overfit if too complex).\n\n\nExamples\ne.g. coefficients in linear regression, weights in neural networks\ne.g. learning rate, number of iterations, number of hidden layers, depth of decision tree, k in kNN and k-means"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#errors",
    "href": "block_2/571_sup_learn/571_sup_learn.html#errors",
    "title": "Supervised Learning",
    "section": "",
    "text": "Generally there are 2 kinds of error:\n\nTraining error (\\(E_{train}\\)): Error on the training data.\nDistribution error/ test error/ generalization error (\\(E_{D}\\)): Error on new data.\n\n\\(E\\_{approx} = E_{D} - E_{train}\\)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#training-validation-and-test-sets",
    "href": "block_2/571_sup_learn/571_sup_learn.html#training-validation-and-test-sets",
    "title": "Supervised Learning",
    "section": "",
    "text": "Training set: The data used to train the model. Used a lot to set parameters.\nValidation set: The data used to evaluate the model during training. Used a few times to set hyperparameters.\nTest set: The data used to evaluate the model after training. Used once to estimate \\(E_{D}\\).\nDeployment: The model is used in the real world.\n\nfrom sklearn.model_selection import train_test_split\n\n# Used directly on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)  # 80%-20% train test split on X and y\n\n# Used on a dataframe\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, random_state=123\n)  # 80%-20% train test split on df\n\n\n\nA method of estimating \\(E_{D}\\) using the training set.\n\nDivide the training set into \\(k\\) folds.\nFor each fold, train on the other \\(k-1\\) folds and evaluate on the current fold.\n\nBenefits:\n\nMore accurate estimate of \\(E_{D}\\). Sometimes just unlucky with train/test split, this helps.\nMore efficient use of data.\n\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# cross_val_score not as comprehensive as cross_validate\n# cross_val_score only returns the scores\ncv_scores = cross_val_score(model, X_train, y_train, cv=10)\n\n# using cross_validate\nscores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n# returns a dictionary with keys: ['fit_time', 'score_time', 'test_score', 'train_score']\npd.DataFrame(scores)\ncross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to evaluate how well the model will generalize to unseen data."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#fundamental-trade-off",
    "href": "block_2/571_sup_learn/571_sup_learn.html#fundamental-trade-off",
    "title": "Supervised Learning",
    "section": "",
    "text": "AKA the bias/ variance trade-off in supervised learning.\n\nBias: Tendency to consistently learn the same wrong thing (high bias = underfitting).\nVariance: Tenency to learn random things irrespective of the real signal (high variance = overfitting).\n\nAs you increase model complexity, \\(E_{train}\\) goes down but \\(E_{approx} = E_{D} - E_{train}\\) goes up."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#baseline",
    "href": "block_2/571_sup_learn/571_sup_learn.html#baseline",
    "title": "Supervised Learning",
    "section": "Baseline",
    "text": "Baseline\n\nA simple, fast, and easily explainable model that is used as a starting point for a more sophisticated model.\n\nMost common baseline for classification is majority class classifier.\nMost common baseline for regression is mean predictor.\nin sklearn, DummyClassifier and DummyRegressor are used to create baselines."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#decision-trees",
    "href": "block_2/571_sup_learn/571_sup_learn.html#decision-trees",
    "title": "Supervised Learning",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nbasic idea: predict using a series of if-then-else questions\ndepth of tree: number of questions asked (hyperparameter)\ndecision boundary: region of feature space where all instances are assigned to the same class\ndecision stump: a decision tree with only one split (depth = 1)\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\n- Easy to interpret and explain.\n- Biased with imbalanced datasets.\n\n\n- Can handle both numerical and categorical data.\n- Greedy splitting algorithm might not find the globally optimal tree.\n\n\n- Can handle multi-output problems.\n- Hard to learn the true relationship between features and target (can only ask yes/no questions).\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3)  # create model object\nclf.fit(X_train, y_train)  # fit model on training data\nclf.score(X_test, y_test)  # score model on test data or use clf.predict(X_test)\n\ncan also use DecisionTreeRegressor for regression problems (continuous target)\n\ndifference:\n\nscore: returns R^2 score (1 is best, 0 is worst)\nleaf nodes: returns average of target values in leaf node\nDecisionTreeClassifier uses entropy and DecisionTreeRegressor uses variance"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#knn",
    "href": "block_2/571_sup_learn/571_sup_learn.html#knn",
    "title": "Supervised Learning",
    "section": "kNN",
    "text": "kNN\n\nkNN is a non-parametric model (no parameters to learn and stores all training data)\nkNN is a lazy learner (no training, just prediction)\n\nslow at prediction time\n\nkNN is a supervised model (needs labels)\nhyperparameters:\n\nk: number of neighbors to consider, smaller k means more complex decision boundary\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n\n\n\n\n\n\n\nPros of k-NNs for Supervised Learning\nCons of k-NNs for Supervised Learning\n\n\n\n\nEasy to understand, interpret.\nCan be potentially VERY slow during prediction time with a large training set.\n\n\nSimple hyperparameter (n_neighbors) controlling the tradeoff\nOften not great test accuracy compared to modern approaches.\n\n\nCan learn very complex functions given enough data.\nDoesn’t work well on datasets with many features or sparse datasets.\n\n\nLazy learning: Takes no time to fit\nFalls apart when # dimensions increase (curse of dimensionality)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#svm-rbf",
    "href": "block_2/571_sup_learn/571_sup_learn.html#svm-rbf",
    "title": "Supervised Learning",
    "section": "SVM RBF",
    "text": "SVM RBF\n\nSVM is a parametric model (needs to learn parameters)\n\nremembers the support vectors\nuses a kernel function to transform the data (RBF, Radial Basis Func, is the default)\n\nDecision boundary only depends on support vectors (smooth)\nhyperparameters:\n\nC: regularization parameter, larger C means more complex\ngamma: kernel coefficient, larger gamma means more complex\n\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(C=10, gamma=0.1)\nsvm.fit(X_train, y_train)\nsvm.score(X_test, y_test)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#scaling-values-using-standardscaler",
    "href": "block_2/571_sup_learn/571_sup_learn.html#scaling-values-using-standardscaler",
    "title": "Supervised Learning",
    "section": "Scaling values using StandardScaler",
    "text": "Scaling values using StandardScaler\n\nin KNN, we need to scale the data (in classification, we don’t need to scale the data)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # create feature trasformer object\nscaler.fit(X_train)  # fitting the transformer on the train split\nX_train_scaled = scaler.transform(X_train)  # transforming the train split\nX_test_scaled = scaler.transform(X_test)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-missing-values-using-simpleimputer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-missing-values-using-simpleimputer",
    "title": "Supervised Learning",
    "section": "Address missing values using SimpleImputer",
    "text": "Address missing values using SimpleImputer\n\nreplace all missing values with the mean/ median of the column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')  # create imputer object\nimputer.fit(X_train_num_only)  # fitting the imputer on the train split\nX_train_num_only_imputed = imputer.transform(X_train_num_only)  # transforming the train split\nX_test_num_only_imputed = imputer.transform(X_test_num_only)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-cataegorical-values-using-onehotencoder",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-cataegorical-values-using-onehotencoder",
    "title": "Supervised Learning",
    "section": "Address cataegorical values using OneHotEncoder",
    "text": "Address cataegorical values using OneHotEncoder\n\nturn categorical values into one-hot encoding\nto get the column names, use get_feature_names(): encoder.get_feature_names().tolist()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nother arguments for OneHotEncoder:\n\nhandle_unknown='ignore' will ignore unknown categories\n\nif you don’t set this, you will get an error if there are unknown categories in the test set\n\nsparse_output=False will return a dense matrix instead of a sparse matrix\n\ndefault is sparse_output=True (returns a sparse matrix - only stores non-zero values)\n\ndrop=\"if_binary\" will drop one of the columns if there are only two categories\n\ndefault is drop=None (no columns are dropped)\ndrop=\"first\" will drop the first column\ndrop=[0, 2] will drop the first and third columns\n\n\n\n\nDiscretizing\n\ne.g turning age into age groups (e.g. child, adult, senior or 0-20, 20-40, 40-60, 60+)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder",
    "title": "Supervised Learning",
    "section": "Address catagorical values using OrdinalEncoder",
    "text": "Address catagorical values using OrdinalEncoder\n\nturn categorical values into ordinal encoding (e.g. low, medium, high)\ndtype=int will make the output an integer\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordered_categories = ['low', 'medium', 'high']\nencoder = OrdinalEncoder(categories=[ordered_categories], dtype=int)  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-bag-of-words-using-countvectorizer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-bag-of-words-using-countvectorizer",
    "title": "Supervised Learning",
    "section": "Address Bag of Words using CountVectorizer",
    "text": "Address Bag of Words using CountVectorizer\n\nturn a string of words into a vector of word counts (e.g., “white couch” -&gt; [“white”: 1, “couch”: 1])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')  # create vectorizer object\nX_train_text_only_vectorized = vectorizer.fit_transform(X_train_text_only)  # fitting and transforming the train split\nX_test_text_only_vectorized = vectorizer.transform(X_test_text_only)  # transforming the test split\n\nParameters:\n\nstop_words='english' will remove common English words (e.g., “the”, “a”, “an”, “and”, “or”, “but”, “not”)\n\ndefault is stop_words=None (no words are removed)\n\nmax_features=100 will only keep the 100 most common words\n\ndefault is max_features=None (all words are kept)\n\n\nhandles unknown words by ignoring them"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#sklearn-summary",
    "href": "block_2/571_sup_learn/571_sup_learn.html#sklearn-summary",
    "title": "Supervised Learning",
    "section": "sklearn summary",
    "text": "sklearn summary\n\n\n\n\n\n\n\n\n\nEstimators\nTransformers\n\n\n\n\nPurpose\nused to fit and predict\nused to change input data\n\n\nUsage\nNeed to fit X_train, y_train\nNeed to fit X_train (no y_train)\n\n\n\nCan score on X_test, y_test\nnothing to score\n\n\nExamples\n- DecisionTreeClassifier\n- StandardScaler\n\n\n\n- KNeighborsClassifier\n- SimpleImputer\n\n\n\n- LogisticRegression\n- OneHotEncoder\n\n\n\n- SVC\n- OrdinalEncoder\n\n\n\n\nDon’t fit with transformer then cross validate with estimator\n\nThis is data leakage (train is influenced by validation)\n\nsolution: Use Sklearn Pipeline!"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#pipeline",
    "href": "block_2/571_sup_learn/571_sup_learn.html#pipeline",
    "title": "Supervised Learning",
    "section": "Pipeline",
    "text": "Pipeline\n\nuse sklearn.pipeline.Pipeline\nmake a pipeline:\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=10)\n)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#column-transformer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#column-transformer",
    "title": "Supervised Learning",
    "section": "Column Transformer",
    "text": "Column Transformer\n\nThis is a transformer that can handle multiple columns\n\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),  # scaling on numeric features\n    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n    # normally OHE is put at the end since it makes new cols\n    (\"drop\", drop_feats),  # drop the drop features\n)\n\nget names of transformers: preprocessor.named_transformers_\nget new column names: preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names()"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#preprocessing-1",
    "href": "block_2/571_sup_learn/571_sup_learn.html#preprocessing-1",
    "title": "Supervised Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nScaling values using StandardScaler\n\nin KNN, we need to scale the data (in classification, we don’t need to scale the data)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # create feature trasformer object\nscaler.fit(X_train)  # fitting the transformer on the train split\nX_train_scaled = scaler.transform(X_train)  # transforming the train split\nX_test_scaled = scaler.transform(X_test)  # transforming the test split\n\n\nAddress missing values using SimpleImputer\n\nreplace all missing values with the mean/ median of the column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')  # create imputer object\nimputer.fit(X_train_num_only)  # fitting the imputer on the train split\nX_train_num_only_imputed = imputer.transform(X_train_num_only)  # transforming the train split\nX_test_num_only_imputed = imputer.transform(X_test_num_only)  # transforming the test split\n\n\nAddress cataegorical values using OneHotEncoder\n\nturn categorical values into one-hot encoding\nto get the column names, use get_feature_names(): encoder.get_feature_names().tolist()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nother arguments for OneHotEncoder:\n\nhandle_unknown='ignore' will ignore unknown categories\n\nif you don’t set this, you will get an error if there are unknown categories in the test set\n\nsparse_output=False will return a dense matrix instead of a sparse matrix\n\ndefault is sparse_output=True (returns a sparse matrix - only stores non-zero values)\n\ndrop=\"if_binary\" will drop one of the columns if there are only two categories\n\ndefault is drop=None (no columns are dropped)\ndrop=\"first\" will drop the first column\ndrop=[0, 2] will drop the first and third columns\n\n\n\n\nDiscretizing\n\ne.g turning age into age groups (e.g. child, adult, senior or 0-20, 20-40, 40-60, 60+)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder-1",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder-1",
    "title": "Supervised Learning",
    "section": "Address catagorical values using OrdinalEncoder",
    "text": "Address catagorical values using OrdinalEncoder\n\nturn categorical values into ordinal encoding (e.g. low, medium, high)\ndtype=int will make the output an integer\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordered_categories = ['low', 'medium', 'high']\nencoder = OrdinalEncoder(categories=[ordered_categories], dtype=int)  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nAddress Bag of Words using CountVectorizer\n\nturn a string of words into a vector of word counts (e.g., “white couch” -&gt; [“white”: 1, “couch”: 1])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')  # create vectorizer object\nX_train_text_only_vectorized = vectorizer.fit_transform(X_train_text_only)  # fitting and transforming the train split\nX_test_text_only_vectorized = vectorizer.transform(X_test_text_only)  # transforming the test split\n\nParameters:\n\nstop_words='english' will remove common English words (e.g., “the”, “a”, “an”, “and”, “or”, “but”, “not”)\n\ndefault is stop_words=None (no words are removed)\n\nmax_features=100 will only keep the 100 most common words\n\ndefault is max_features=None (all words are kept)\n\nbinary=True will only keep 0 or 1 for each word (instead of the count)\n\ndefault is binary=False (the count is kept)\n\n\nhandles unknown words by ignoring them\n\n\nBreaking Golden Rule\n\nIf we know fixed categories (i.e., provinces in Canada), we can break the golden rule and pass the list of known/possible categories\n\n\n\n\nsklearn summary\n\n\n\n\n\n\n\n\n\nEstimators\nTransformers\n\n\n\n\nPurpose\nused to fit and predict\nused to change input data\n\n\nUsage\nNeed to fit X_train, y_train\nNeed to fit X_train (no y_train)\n\n\n\nCan score on X_test, y_test\nnothing to score\n\n\nExamples\n- DecisionTreeClassifier\n- StandardScaler\n\n\n\n- KNeighborsClassifier\n- SimpleImputer\n\n\n\n- LogisticRegression\n- OneHotEncoder\n\n\n\n- SVC\n- OrdinalEncoder\n\n\n\n\nDon’t fit with transformer then cross validate with estimator\n\nThis is data leakage (train is influenced by validation)\n\nsolution: Use Sklearn Pipeline!\n\n\n\nPipeline\n\nuse sklearn.pipeline.Pipeline\nmake a pipeline:\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=10)\n)\n\nscores = cross_validate(pipe_knn, X_train, y_train, return_train_score=True)\npipe_knn.fit(X_train, y_train)\n\n\nColumn Transformer\n\nThis is a transformer that can handle multiple columns\n\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),  # scaling on numeric features\n    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n    # normally OHE is put at the end since it makes new cols\n    (\"drop\", drop_feats),  # drop the drop features\n)\n\nget names of transformers: preprocessor.named_transformers_\nget new column names: preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names()"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#hyperparamter-optimization",
    "href": "block_2/571_sup_learn/571_sup_learn.html#hyperparamter-optimization",
    "title": "Supervised Learning",
    "section": "Hyperparamter Optimization",
    "text": "Hyperparamter Optimization\n\nMethods\n\nManual\n\nTakes a lot of time\nintuition is not always correct\nsome hyperparameters work together\n\nAutomated\n\nGrid search\n\n\n\n\nGrid Search\n\nExhaustive search over specified parameter values for an estimator\n\nruns \\(n^m\\) CV for m hyperparameters and n values for each parameter\n\nAfter finding best parameter, it trains/fits the model on the whole training set\n\n\nParameters:\n\nGridSearchCV(estimator, param_grid, scoring=None, cv=None, n_jobs=None))\nestimator: estimator object\nparam_grid: dictionary with parameters names as keys and lists of parameter settings to try as values\n\nuses __ syntax to specify parameters of the estimator\ne.g:\n\ncolumntransformer__countvectorizer__max_features: max features of count vectorizer in column transformer\nsvc__gamma: gamma of SVC\n\n\nscoring: scoring method\ncv: cross-validation method\nn_jobs: number of jobs to run in parallel (-1 means use all processors)\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(preprocessor, SVC())\n\nparam_grid = {\n    \"columntransformer__countvectorizer__max_features\": [100, 200, 400, 800, 1000, 2000],\n    \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n    \"svc__C\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n}\n\n# Create a grid search object\ngs = GridSearchCV(pipe_svm, param_grid=param_grid, cv=5, n_jobs=-1)\ngs.fit(X_train, y_train)\n\ngs.best_score_ # returns the best score\ngs.best_params_ # returns the best parameters\n# Returns a dataframe of all the results\npd.DataFrame(random_search.cv_results_)[\n    [\n        \"mean_test_score\",\n        \"param_columntransformer__countvectorizer__max_features\",\n        \"param_svc__gamma\",\n        \"param_svc__C\",\n        \"mean_fit_time\",\n        \"rank_test_score\",\n    ]\n].set_index(\"rank_test_score\").sort_index().T\n\n# Can score on the test set\ngs.score(X_test, y_test)\n\n\n\nRandom Search\n\nPicks random values for the hyperparameters according to a distribution\nonly runs n_iter CV\n\n\nParameters\n\nRandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, cv=None, n_jobs=None))\nestimator: estimator object\nparam_distributions: dictionary with parameters names as keys and distributions or lists of parameters to try\n\ncan also pass param_grid from grid search (but does not exhaustively search)\n\nn_iter: number of parameter settings that are sampled\nscoring: scoring method\ncv: cross-validation method\nn_jobs: number of jobs to run in parallel (-1 means use all processors)\n\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"columntransformer__countvectorizer__max_features\": randint(100, 2000),\n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\n\nrs = RandomizedSearchCV(\n    pipe_svm,\n    param_distributions=param_dist,\n    n_iter=10,\n    cv=5,\n    n_jobs=-1,\n    random_state=42,\n)\n\n\n\nRandom vs Grid Search\n\nAdvantages of Random Search\n\nFaster\nAdding hyperparameters that do not influence the performance does not decrease the performance\nWorks better when some hyperparameters are more important than others\nrecommended more than grid search"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#naive-bayes",
    "href": "block_2/571_sup_learn/571_sup_learn.html#naive-bayes",
    "title": "Supervised Learning",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nBayes’ Theorem\n\\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\n\nBasic idea\n\nWe have a set of classes (e.g. spam or not spam)\nWe have a set of features (e.g. words in an email)\n\nWe want to find the probability of a class given a set of features.\n\\[ P(C|F_1, F_2, ..., F_n) = \\frac{P(F_1, F_2, ..., F_n|C)P(C)}{P(F_1, F_2, ..., F_n)} \\]\n\n\nNaive Bayes\n\nBag of words model (order of words doesn’t matter)\nAssume that all features are conditionally independent of each other (naive assumption)\nThis allows us to simplify the equation to:\n\n\\[ P(C|F*1, F_2, ..., F_n) \\approx \\frac{P(C)* \\prod P(F_i|C)}{P(F_1, F_2, ..., F_n)} \\]\n\n\nLaplace Smoothing\n\nIf a word is not in the training set, then the probability of that word given a class is 0\nThis will cause the entire probability to be 0\nWe can fix this by adding 1 to the numerator and adding the number of words to the denominator\n\n\\[ P(F_i|C) = \\frac{count(F_i, C) + 1}{count(C) + |V|} \\]\nwhere V is the number of possible word values\n\n\nSklearn Implementation\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe_nb = make_pipeline(CountVectorizer(), MultinomialNB())\nresults_dict[\"Naive Bayes\"] = mean_std_cross_val_scores(\n    pipe_nb, X_train, y_train, return_train_score=True\n)\n\nMultinomialNB generally works better than BernoulliNB, especially for large text datasets\n\nBernoulliNB assumes that the features are binary (e.g. 0 or 1)\nMultinomialNB assumes that the features are counts (e.g. 0, 1, 2, 3, …)\n\nParameters:\n\nalpha is the Laplace smoothing parameter (actually hyperparameter), default is alpha=1.0\n\nHigh alpha means more smoothing =&gt; underfitting\nLow alpha means less smoothing =&gt; overfitting\n\n\n\n\n\nContinuous Features\n\nWe can use a Gaussian Naive Bayes model for continuous features\nThis assumes that the features are normally distributed\n\nIf not, can use sklearn.preprocessing.PowerTransformer to transform the data to be more normal\n\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# view\nmodel.theta_  # mean of each feature per class\nmodel.sigma_  # variance of each feature per class\nmodel.var_ # overall variance of each feature\nmodel.class_prior_  # prior probability of each class\n\nmodel.predict_proba(X_test)  # probability of each class"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#linear-models",
    "href": "block_2/571_sup_learn/571_sup_learn.html#linear-models",
    "title": "Supervised Learning",
    "section": "Linear Models",
    "text": "Linear Models\n\nmake predictions using a linear function of the input features\ndecision boundary is a hyperplane\n\nif 2d, decision boundary is a line\nuncertain near the decision boundary\n\nLimitations:\n\ncan only learn linear decision boundaries\ncan only learn linear functions of the input features\n\n\n\nLinear Regression\n\nMain idea: find the line that minimizes the sum of squared errors\nComponents:\n\ninput features (d features)\ncoefficients (d coefficients)\nintercept/ bias (1 intercept)\n\nNormally has d+1 parameters (one for each feature plus the intercept)\nMakes d-1 hyperplanes (separating lines) in d dimensions\nMore complex normally means the coefficients are larger\nRaw output score can be used to calculate probability score for a given prediction\nSCALING IS IMPORTANT\n\nIf features are on different scales, the coefficients will be on different scales\n\n\n\nRidge\n\\[ \\min\\_{w} ||Xw - y||\\_2^2 + \\alpha ||w||\\_2^2 \\]\n\nL2 regularization\nHyperparameters:\n\nalpha: regularization strength\n\nlarger values =&gt; more regularization =&gt; simpler model =&gt; underfitting\nmore regularization =&gt; smaller coefficients =&gt; less sensitive to changes in input features (outliers)\n\n\n\nfrom sklearn.linear_model import Ridge\n\npipe = make_pipeline(StandardScaler(), Ridge())\nscores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n\ncoeffs = pipe_ridge.named_steps[\"ridge\"].coef_ # view coefficients\n# coeffs.shape = (n_features,), one coefficient for each feature\nintercept = pipe_ridge.named_steps[\"ridge\"].intercept_ # view intercept/ bias\n\n\nLaso\n\\[ \\min\\_{w} ||Xw - y||\\_2^2 + \\alpha ||w||\\_1 \\]\n\nL1 regularization\nHyperparameters:\n\nalpha: regularization strength\n\nlarger values =&gt; more regularization =&gt; simpler model =&gt; underfitting\n\n\n\n\n\n\nLogistic Regression\n\nMain idea: use linear regression to predict the probability of an event\nApplies a “threshold” to the raw output score to make a prediction -&gt; decides whether to predict 0/1 or -1/1\nComponents:\n\ninput features (d features)\ncoefficients (d coefficients)\nintercept/ bias (1 intercept)\nthreshold r (1 threshold)\n\nHyperparameters:\n\nC: inverse of regularization strength\n\nlarger values =&gt; less regularization =&gt; more complex model =&gt; overfitting\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nscores = cross_validate(lr, X_train, y_train, return_train_score=True)\n\n# access coefficients and intercept\nlr.coef_ # shape = (n_classes, n_features)\nlr.intercept_ # shape = (n_classes,)\n\nlr.classes_ # array of classes\n\npredict_proba returns the probability of each class\n\nfor binary classification, returns both classes (although one is redundant)\nbased on the order of lr.classes_\nsum of probabilities for each sample is 1\n\npredict returns the class with the highest probability\n\n\nsigmoid function\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n\nturns -inf to 0 and inf to 1 \n\n\n\n\nLinear SVM\n\nMain idea: find the line that maximizes the margin between the decision boundary and the closest points\n\nfrom sklearn.svm import SVC\n\nlinear_svc = SVC(kernel=\"linear\")\nscores = cross_validate(linear_svc, X_train, y_train, return_train_score=True)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#multi-class-meta-strategies",
    "href": "block_2/571_sup_learn/571_sup_learn.html#multi-class-meta-strategies",
    "title": "Supervised Learning",
    "section": "Multi-class, meta-strategies",
    "text": "Multi-class, meta-strategies\n\nCan do multiclass naturally: KNN, decision trees\n2 hacky ways to use binary classifiers for multi-class classification:\n\n\nOne-vs-Rest (OVR)\n\nTrain a binary classifier for each class\n\ncreates binary linear classifiers separating each class from the rest (i.e blue vs rest, red vs rest, green vs rest)\n\nClassify by choosing the class with the highest probability\n\n\ne.g. a point on (0, -5) would get:\n\nlr.coef_ would give 3x2 array (3 classes, 2 features)\nlr.intercept_ would give 3x1 array (3 classes, 1 intercept)\nGet score with test_points[4]@lr.coef_.T + lr.intercept_ return array size 3, choose the class with the highest score\n\n\n\nOne-vs-One (OVO)\n\nTrain a binary classifier for each pair of classes\n\ncreates binary linear classifiers separating each class from each other class (i.e blue vs red, blue vs green, red vs green)\ntrains \\(\\frac{n(n-1)}{2}\\) classifiers\n\ncount the number of times each class wins\nClassify by choosing the class with the most wins\n\n\n\nUsing this in Python\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n\nmodel = OneVsOneClassifier(LogisticRegression())\n%timeit model.fit(X_train_multi, y_train_multi);\n\nmodel = OneVsRestClassifier(LogisticRegression())\n%timeit model.fit(X_train_multi, y_train_multi);"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html",
    "href": "block_2/512_algs_ds/512_algs_ds.html",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Big O\nname\nchange in runtime if I double \\(n\\)?\n\n\n\n\n\\(O(1)\\)\nconstant\nsame\n\n\n\\(O(\\log n)\\)\nlogarithmic\nincreased by a constant\n\n\n\\(O(\\sqrt{n})\\)\nsquare root\nincreased by roughly 1.4x\n\n\n\\(O(n)\\)\nlinear\n2x\n\n\n\\(O(n \\log n)\\)\nlinearithmic\nroughly 2x\n\n\n\\(O(n^2)\\)\nquadratic\n4x\n\n\n\\(O(n^3)\\)\ncubic\n8x\n\n\n\\(O(n^k)\\)\npolynomial\nincrease by a factor of \\(2^k\\)\n\n\n\\(O(2^n)\\)\nexponential\nsquared\n\n\n\nsorted from fastest to slowest\nIf mult/ div by contant c: \\(log_c(n)\\) e.g. for (int i = 0; i &lt; n; i *= 2) If add/ sub by constant c: \\(n/c\\) e.g. for (int i = 0; i &lt; n; i += 2)\n\nWe write \\(O(f(n))\\) for some function \\(f(n)\\).\nYou get the doubling time by taking \\(f(2n)/f(n)\\).\nE.g. if \\(f(n)=n^3\\), then \\(f(2n)/f(n)=(2n)^3/n^3=8\\).\n\nSo if you double \\(n\\), the running time goes up 8x.\n\nFor \\(O(2^n)\\), increasing \\(n\\) by 1 causes the runtime to double!\n\nNote: these are common cases of big O, but this list is not exhaustive.\n\n\n\n\n\n\nSpace complexity is the amount of memory used by an algorithm.\nWe can use big O notation to describe space complexity.\n\n\n\n\nrange() is a generator, so it doesn’t take up memory\nlist(range()) is a list, so it takes up memory\nnp.arange() is an array, so it takes up memory\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nLinear Search\nBinary Search\n\n\n\n\nPrinciple\nSequentially checks each element until a match is found or end is reached.\nRepeatedly divides in half the portion of the list that could contain the item until you’ve narrowed down the possible locations to just one.\n\n\nBest-case Time Complexity\n(O(1))\n(O(1))\n\n\nSpace Complexity\n(O(1))\n(O(1))\n\n\nWorks on\nUnsorted and sorted lists\nSorted lists only\n\n\n\n\n\ndef linear_search(arr, x):\n    for i in range(len(arr)):\n        if arr[i] == x:\n            return i\n    return -1  # not found\n\n# Example usage:\narr = [10, 20, 80, 30, 60, 50, 110, 100, 130, 170]\nx = 110\nresult = linear_search(arr, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\ndef binary_search(arr, l, r, x):\n    while l &lt;= r:\n        mid = l + (r - l) // 2\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n        # If x is greater, ignore left half\n        elif arr[mid] &lt; x:\n            l = mid + 1\n        # If x is smaller, ignore right half\n        else:\n            r = mid - 1\n    # Element was not present\n    return -1\n\n# Example usage:\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nx = 70\nresult = binary_search(arr, 0, len(arr)-1, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorst-case Time Complexity\nSpace Complexity\nDescription\nViz\n\n\n\n\nInsertion Sort\n(O(n^2))\n(O(1))\nBuilds the final sorted list one item at a time. It takes one input element per iteration and finds its correct position in the sorted list.\n\n\n\nSelection Sort\n(O(n^2))\n(O(1))\nDivides the input list into two parts: a sorted and an unsorted sublist. It repeatedly selects the smallest (or largest) element from the unsorted sublist and moves it to the end of the sorted sublist.\n\n\n\nBubble Sort\n(O(n^2))\n(O(1))\nRepeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The process is repeated for each item.\n\n\n\nMerge Sort\n(O(n n))\n(O(n))\nDivides the unsorted list into n sublists, each containing one element, then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining.\n\n\n\nHeap Sort\n(O(n n))\n(O(1))\nConverts the input data into a heap data structure. It then extracts the topmost element (max or min) and reconstructs the heap, repeating this process until the heap is empty.\n\n\n\n\ngifs from https://emre.me/algorithms/sorting-algorithms/\n\n\n\n\n\n\nHashing is a technique that is used to uniquely identify a specific object from a group of similar objects.\nin python: hash()\nonly immutable objects can be hashed\n\nlists, sets, and dictionaries are mutable and cannot be hashed\ntuples are immutable and can be hashed\n\n\n\n\n\n\nCreating dictionaries:\n\nx = {}\n\nx = {'a': 1, 'b': 2}\n\nx = dict()\n\nx = dict(a=1, b=2)\nx = dict([('a', 1), ('b', 2)])\nx = dict(zip(['a', 'b'], [1, 2]))\nx = dict({'a': 1, 'b': 2})\n\n\nAccessing values:\n\nx['a']: if key is not found, raises KeyError\nx.get('a', 0): returns 0 if key is not found\n\n\n\n\n\n\ndefaultdict is a subclass of dict that returns a default value when a key is not found\n\nfrom collections import defaultdict\nd = defaultdict(int)\n\nd['a'] returns 0\n\nd = defaultdict(list)\n\nd['a'] returns []\nnot list() because list() is a function that returns an empty list, list is a type\n\nd = defaultdict(set)\n\nd['a'] returns set()\n\nd = defaultdict(lambda: \"hello I am your friendly neighbourhood default value\")\n\nd['a'] returns \"hello I am your friendly neighbourhood default value\"\n\n\n\n\n\n\n\nCounter is a subclass of dict that counts the number of occurrences of an element in a list\n\nfrom collections import Counter\nc = Counter(['a', 'b', 'c', 'a', 'b', 'b'])\n\nc['a'] returns 2\nc['b'] returns 3\nc['c'] returns 1\nc['d'] returns 0\n\nc = Counter({'a': 2, 'b': 3, 'c': 1})\nc = Counter(a=2, b=3, c=1)\n\nother functions:\n\nc.most_common(2) returns the 2 most common elements in the list: [('b', 3), ('a', 2)]\n\n\n\n\n\n\n\ncontains vertices (or nodes) and edges\nuse networkx to create graphs in Python\n\nimport networkx as nx\n\nG = nx.Graph()  # create empty graph\nG.add_node(\"YVR\")  # add node 1\nG.add_node(\"YYZ\")  # add node 2\nG.add_node(\"YUL\")  # add node 3\n\nG.add_edge(\"YVR\", \"YYZ\", weight=4)  # add edge between node 1 and node 2\nG.add_edge(\"YVR\", \"YUL\", weight=5)  # add edge between node 1 and node 3\n\nnx.draw(G, with_labels=True) # draw graph but random layout\nnx.draw(G, with_labels=True, pos=nx.spring_layout(G, seed=5)) # not random layout\n\n\n\ndirected graph: edges have direction\n\nnx.DiGraph()\n\n\n\n\n\n\nDegree: number of edges connected to a node\nPath: sequence of nodes connected by edges\nConnected: graph where there is a path between every pair of nodes\nComponent: connected subgraph\n\n\n\n\n\n\n\ndef bfs(graph, start, search):\n    # graph: networkx graph\n\n    visited = set()\n    queue = [start]\n    while queue:\n        node = queue.pop(0)\n        if node in visited:\n            continue\n        visited.add(node)\n        if node == search:\n            return True\n        for neighbour in g.neighbors(node):\n            queue.append(neighbour)\n\n    return False\n\n\n\nFIFO (first in first out)\n\nqueue = []\nqueue.append(1)  # add to end of queue\nqueue.append(2)  # add to end of queue\nqueue.pop(0)  # remove from front of queue\n\n\n\n\n\n\n\nLIFO (last in first out)\n\nstack = []\nstack.append(1)  # add to end of stack\nstack.append(2)  # add to end of stack\nstack.pop()  # remove from end of stack\n\n\n\n\n\nAdjacency List\n\nlists of all the edges in the graph\nspace complexity: O(E)\nstill need to store all the nodes\n\nAdjacency Matrix\n\nmatrix of 0s and 1s with size V x V\ndense matrix space complexity: O(V^2)\nsparse matrix space complexity: O(E)\n\nCan do both directed and undirected graphs\n\n\nnetworkx uses scipy sparse matrix\n\ncreate sparse matrix: scipy.sparse.csr_matrix(x)\nneed to acces using matrix[row, col]\n\nmatrix[row][col] will not work, in np it will work\n\nto sum all the rows: matrix.sum(axis=1)\n\ncannot do np.sum(matrix, axis=1)\nor do matrix.getnnz(axis=1) to get number of non-zero elements in each row\n\nto find vertex: np.argmax(matrix.getnnz(axis=1))\nto find # max edges: np.max(matrix.getnnz(axis=1))\n\n\n\n\n\n\n\n\n\nLinear programming is a method to achieve the best outcome (e.g. maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships\n\n\n\n\nTo specify an optimization problem, we need to specify a few things:\n\nA specification of the space of possible inputs.\nAn objective function which takes an input and computes a score.\n(Optional) A set of constraints, which take an input and return true/false.\n\ncan only get same or worse score than not having constraints\n\nAre we maximizing or minimizing?\n\n\n\n\n\nPuLP is an LP modeler written in Python\ncontinuous problems are easier to solve than discrete problems\n\ndiscrete problems might not be “optimal”\n\nExample of discrete problem: assigning TAs to courses\n\n# Define the problem\nprob = pulp.LpProblem(\"TA-assignments\", pulp.LpMaximize)\n\n# Define the variables\nx = pulp.LpVariable.dicts(\"x\", (TAs, courses), 0, 1, pulp.LpInteger)\n\n# add constraints\nfor course in courses:\n    prob += pulp.lpSum(x[ta][course] for ta in TAs) == 1 # += adds constraint\n\n# add objective\nprob += pulp.lpSum(x[ta][course] * happiness[ta][course] for ta in TAs for course in courses)\n\n# solve\nprob.solve()\n# prob.solve(pulp.apis.PULP_CBC_CMD(msg=0)) # to suppress output\n\n# check status\npulp.LpStatus[prob.status] # 'Optimal'\n\n# print results\nfor ta in TAs:\n    for course in courses:\n        if x[ta][course].value() == 1.0:\n            print(f\"{ta} is assigned to {course}\")\n\n\n\n\nDynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc).\nDynamic programming only works on problems with optimal substructure and overlapping subproblems.\n\noptimal substructure: optimal solution can be constructed from optimal solutions of its subproblems\noverlapping subproblems: subproblems recur many times\n\nDynamic programming is usually applied to optimization problems."
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#big-o-notation",
    "href": "block_2/512_algs_ds/512_algs_ds.html#big-o-notation",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Big O\nname\nchange in runtime if I double \\(n\\)?\n\n\n\n\n\\(O(1)\\)\nconstant\nsame\n\n\n\\(O(\\log n)\\)\nlogarithmic\nincreased by a constant\n\n\n\\(O(\\sqrt{n})\\)\nsquare root\nincreased by roughly 1.4x\n\n\n\\(O(n)\\)\nlinear\n2x\n\n\n\\(O(n \\log n)\\)\nlinearithmic\nroughly 2x\n\n\n\\(O(n^2)\\)\nquadratic\n4x\n\n\n\\(O(n^3)\\)\ncubic\n8x\n\n\n\\(O(n^k)\\)\npolynomial\nincrease by a factor of \\(2^k\\)\n\n\n\\(O(2^n)\\)\nexponential\nsquared\n\n\n\nsorted from fastest to slowest\nIf mult/ div by contant c: \\(log_c(n)\\) e.g. for (int i = 0; i &lt; n; i *= 2) If add/ sub by constant c: \\(n/c\\) e.g. for (int i = 0; i &lt; n; i += 2)\n\nWe write \\(O(f(n))\\) for some function \\(f(n)\\).\nYou get the doubling time by taking \\(f(2n)/f(n)\\).\nE.g. if \\(f(n)=n^3\\), then \\(f(2n)/f(n)=(2n)^3/n^3=8\\).\n\nSo if you double \\(n\\), the running time goes up 8x.\n\nFor \\(O(2^n)\\), increasing \\(n\\) by 1 causes the runtime to double!\n\nNote: these are common cases of big O, but this list is not exhaustive."
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#space-complexity",
    "href": "block_2/512_algs_ds/512_algs_ds.html#space-complexity",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Space complexity is the amount of memory used by an algorithm.\nWe can use big O notation to describe space complexity.\n\n\n\n\nrange() is a generator, so it doesn’t take up memory\nlist(range()) is a list, so it takes up memory\nnp.arange() is an array, so it takes up memory"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#searching",
    "href": "block_2/512_algs_ds/512_algs_ds.html#searching",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Feature\nLinear Search\nBinary Search\n\n\n\n\nPrinciple\nSequentially checks each element until a match is found or end is reached.\nRepeatedly divides in half the portion of the list that could contain the item until you’ve narrowed down the possible locations to just one.\n\n\nBest-case Time Complexity\n(O(1))\n(O(1))\n\n\nSpace Complexity\n(O(1))\n(O(1))\n\n\nWorks on\nUnsorted and sorted lists\nSorted lists only\n\n\n\n\n\ndef linear_search(arr, x):\n    for i in range(len(arr)):\n        if arr[i] == x:\n            return i\n    return -1  # not found\n\n# Example usage:\narr = [10, 20, 80, 30, 60, 50, 110, 100, 130, 170]\nx = 110\nresult = linear_search(arr, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\ndef binary_search(arr, l, r, x):\n    while l &lt;= r:\n        mid = l + (r - l) // 2\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n        # If x is greater, ignore left half\n        elif arr[mid] &lt; x:\n            l = mid + 1\n        # If x is smaller, ignore right half\n        else:\n            r = mid - 1\n    # Element was not present\n    return -1\n\n# Example usage:\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nx = 70\nresult = binary_search(arr, 0, len(arr)-1, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#sorting",
    "href": "block_2/512_algs_ds/512_algs_ds.html#sorting",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Algorithm\nWorst-case Time Complexity\nSpace Complexity\nDescription\nViz\n\n\n\n\nInsertion Sort\n(O(n^2))\n(O(1))\nBuilds the final sorted list one item at a time. It takes one input element per iteration and finds its correct position in the sorted list.\n\n\n\nSelection Sort\n(O(n^2))\n(O(1))\nDivides the input list into two parts: a sorted and an unsorted sublist. It repeatedly selects the smallest (or largest) element from the unsorted sublist and moves it to the end of the sorted sublist.\n\n\n\nBubble Sort\n(O(n^2))\n(O(1))\nRepeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The process is repeated for each item.\n\n\n\nMerge Sort\n(O(n n))\n(O(n))\nDivides the unsorted list into n sublists, each containing one element, then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining.\n\n\n\nHeap Sort\n(O(n n))\n(O(1))\nConverts the input data into a heap data structure. It then extracts the topmost element (max or min) and reconstructs the heap, repeating this process until the heap is empty.\n\n\n\n\ngifs from https://emre.me/algorithms/sorting-algorithms/"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#hashmap",
    "href": "block_2/512_algs_ds/512_algs_ds.html#hashmap",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Hashing is a technique that is used to uniquely identify a specific object from a group of similar objects.\nin python: hash()\nonly immutable objects can be hashed\n\nlists, sets, and dictionaries are mutable and cannot be hashed\ntuples are immutable and can be hashed\n\n\n\n\n\n\nCreating dictionaries:\n\nx = {}\n\nx = {'a': 1, 'b': 2}\n\nx = dict()\n\nx = dict(a=1, b=2)\nx = dict([('a', 1), ('b', 2)])\nx = dict(zip(['a', 'b'], [1, 2]))\nx = dict({'a': 1, 'b': 2})\n\n\nAccessing values:\n\nx['a']: if key is not found, raises KeyError\nx.get('a', 0): returns 0 if key is not found\n\n\n\n\n\n\ndefaultdict is a subclass of dict that returns a default value when a key is not found\n\nfrom collections import defaultdict\nd = defaultdict(int)\n\nd['a'] returns 0\n\nd = defaultdict(list)\n\nd['a'] returns []\nnot list() because list() is a function that returns an empty list, list is a type\n\nd = defaultdict(set)\n\nd['a'] returns set()\n\nd = defaultdict(lambda: \"hello I am your friendly neighbourhood default value\")\n\nd['a'] returns \"hello I am your friendly neighbourhood default value\"\n\n\n\n\n\n\n\nCounter is a subclass of dict that counts the number of occurrences of an element in a list\n\nfrom collections import Counter\nc = Counter(['a', 'b', 'c', 'a', 'b', 'b'])\n\nc['a'] returns 2\nc['b'] returns 3\nc['c'] returns 1\nc['d'] returns 0\n\nc = Counter({'a': 2, 'b': 3, 'c': 1})\nc = Counter(a=2, b=3, c=1)\n\nother functions:\n\nc.most_common(2) returns the 2 most common elements in the list: [('b', 3), ('a', 2)]"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#graphs",
    "href": "block_2/512_algs_ds/512_algs_ds.html#graphs",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "contains vertices (or nodes) and edges\nuse networkx to create graphs in Python\n\nimport networkx as nx\n\nG = nx.Graph()  # create empty graph\nG.add_node(\"YVR\")  # add node 1\nG.add_node(\"YYZ\")  # add node 2\nG.add_node(\"YUL\")  # add node 3\n\nG.add_edge(\"YVR\", \"YYZ\", weight=4)  # add edge between node 1 and node 2\nG.add_edge(\"YVR\", \"YUL\", weight=5)  # add edge between node 1 and node 3\n\nnx.draw(G, with_labels=True) # draw graph but random layout\nnx.draw(G, with_labels=True, pos=nx.spring_layout(G, seed=5)) # not random layout\n\n\n\ndirected graph: edges have direction\n\nnx.DiGraph()\n\n\n\n\n\n\nDegree: number of edges connected to a node\nPath: sequence of nodes connected by edges\nConnected: graph where there is a path between every pair of nodes\nComponent: connected subgraph"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#graph-searching",
    "href": "block_2/512_algs_ds/512_algs_ds.html#graph-searching",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "def bfs(graph, start, search):\n    # graph: networkx graph\n\n    visited = set()\n    queue = [start]\n    while queue:\n        node = queue.pop(0)\n        if node in visited:\n            continue\n        visited.add(node)\n        if node == search:\n            return True\n        for neighbour in g.neighbors(node):\n            queue.append(neighbour)\n\n    return False\n\n\n\nFIFO (first in first out)\n\nqueue = []\nqueue.append(1)  # add to end of queue\nqueue.append(2)  # add to end of queue\nqueue.pop(0)  # remove from front of queue\n\n\n\n\n\n\n\nLIFO (last in first out)\n\nstack = []\nstack.append(1)  # add to end of stack\nstack.append(2)  # add to end of stack\nstack.pop()  # remove from end of stack\n\n\n\n\n\nAdjacency List\n\nlists of all the edges in the graph\nspace complexity: O(E)\nstill need to store all the nodes\n\nAdjacency Matrix\n\nmatrix of 0s and 1s with size V x V\ndense matrix space complexity: O(V^2)\nsparse matrix space complexity: O(E)\n\nCan do both directed and undirected graphs\n\n\nnetworkx uses scipy sparse matrix\n\ncreate sparse matrix: scipy.sparse.csr_matrix(x)\nneed to acces using matrix[row, col]\n\nmatrix[row][col] will not work, in np it will work\n\nto sum all the rows: matrix.sum(axis=1)\n\ncannot do np.sum(matrix, axis=1)\nor do matrix.getnnz(axis=1) to get number of non-zero elements in each row\n\nto find vertex: np.argmax(matrix.getnnz(axis=1))\nto find # max edges: np.max(matrix.getnnz(axis=1))"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#linear-programming",
    "href": "block_2/512_algs_ds/512_algs_ds.html#linear-programming",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Linear programming is a method to achieve the best outcome (e.g. maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#defining-the-problem",
    "href": "block_2/512_algs_ds/512_algs_ds.html#defining-the-problem",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "To specify an optimization problem, we need to specify a few things:\n\nA specification of the space of possible inputs.\nAn objective function which takes an input and computes a score.\n(Optional) A set of constraints, which take an input and return true/false.\n\ncan only get same or worse score than not having constraints\n\nAre we maximizing or minimizing?"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#using-python-pulp",
    "href": "block_2/512_algs_ds/512_algs_ds.html#using-python-pulp",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "PuLP is an LP modeler written in Python\ncontinuous problems are easier to solve than discrete problems\n\ndiscrete problems might not be “optimal”\n\nExample of discrete problem: assigning TAs to courses\n\n# Define the problem\nprob = pulp.LpProblem(\"TA-assignments\", pulp.LpMaximize)\n\n# Define the variables\nx = pulp.LpVariable.dicts(\"x\", (TAs, courses), 0, 1, pulp.LpInteger)\n\n# add constraints\nfor course in courses:\n    prob += pulp.lpSum(x[ta][course] for ta in TAs) == 1 # += adds constraint\n\n# add objective\nprob += pulp.lpSum(x[ta][course] * happiness[ta][course] for ta in TAs for course in courses)\n\n# solve\nprob.solve()\n# prob.solve(pulp.apis.PULP_CBC_CMD(msg=0)) # to suppress output\n\n# check status\npulp.LpStatus[prob.status] # 'Optimal'\n\n# print results\nfor ta in TAs:\n    for course in courses:\n        if x[ta][course].value() == 1.0:\n            print(f\"{ta} is assigned to {course}\")"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#dynamic-programming",
    "href": "block_2/512_algs_ds/512_algs_ds.html#dynamic-programming",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc).\nDynamic programming only works on problems with optimal substructure and overlapping subproblems.\n\noptimal substructure: optimal solution can be constructed from optimal solutions of its subproblems\noverlapping subproblems: subproblems recur many times\n\nDynamic programming is usually applied to optimization problems."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html",
    "href": "block_2/552_stat_inter/552_stat_inter.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\npoint_estimate\nA summary statistic calculated from a random sample that estimates an unknown population parameter of interest.\n\n\npopulation\nThe entire set of entities objects of interest.\n\n\npopulation_parameter\nA numerical summary value about the population.\n\n\nsample\nA collected subset of observations from a population.\n\n\nobservation\nA quantity or quality (or a set of these) from a single member of a population.\n\n\nsampling_distribution\nA distribution of point estimates, where each point estimate was calculated from a different random sample coming from the same population.\n\n\n\n\nTrue population parameter is denoted with Greek letters. (e.g. μ for mean)\nEstimated population parameter is denoted with Greek letters with a hat. (e.g. μ̂ for mean)\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nDescriptive\nSummarizes a characteristic of a set of data without interpretation.\n- What is the frequency of bacterial illnesses in a data set?  - How many people live in each US state?\n\n\nExploratory\nAnalyzes data to identify patterns, trends, or relationships between variables.\n- Do certain diets correlate with bacterial illnesses?  - Does air pollution correlate with life expectancy in different US regions?\n\n\nInferential\nAnalyzes patterns, trends, or relationships in a representative sample to quantify applicability to the whole population. Estimation with associated randomness.\n- Is eating 5 servings of fruits and vegetables associated with fewer bacterial illnesses?  - Is gestational length different for first born babies?\n\n\nPredictive\nAims to predict measurements or labels for individuals. Not focused on causes but on predictions.\n- How many viral illnesses will someone have next year?  - What political party will someone vote for in the next US election?\n\n\nCausal\nInquires if changing one factor will change another factor in a population. Sometimes design allows for causal interpretation.\n- Does eating 5 servings of fruits and vegetables cause fewer bacterial illnesses?  - Does smoking lead to cancer?\n\n\nMechanistic\nSeeks to explain the underlying mechanism of observed patterns or relationships.\n- How do changes in diet reduce bacterial illnesses?  - How does airplane wing design affect air flow and decrease drag?\n\n\n\n\n\n\nDefine the population of interest.\nSelect the right sampling method according to the specific characteristics of our population of interest.\nSelect our sample size (Power Analysis).\nCollect the sampled data.\nMeasure and calculate the sample statistic.\nInfer the population value based on this sample statistic while accounting for sampling uncertainty.\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\n\\(\\pi_E\\): population proportion\n\\(\\hat{\\pi}_E\\): sample proportion\n\n\n\\(\\mu\\): population mean\n\\(\\bar{\\mu}\\): sample mean\n\n\n\nPopulation Distribution:\n\nThe sample distribution is of a similar shape to the population distribution.\nThe sample point estimates are identical to the values for the true population parameter we are trying to estimate.\n\nSample Distribution of 1 Sample:\n\nTaking a random sample and calculating a point estimate is a “good guess” of the unknown population parameter you are interested in.\nAs the sample size increases:\n\nthe sampling distribution becomes narrower.\nmore sample point estimates are closer to the true population mean.\nthe sampling distribution appears more bell-shaped.\n\n\nSampling Distribution of Sample Means:\n\nThe sampling distribution is centered at the true population mean.\nMost sample means are at or very near the same value as the true population mean.\nThe sample distribution (if representative) is an estimate of the population distribution.\nThe sampling distribution of the sample means is not necessarily the same shape as the distribution of the population distribution and tends to be more symmetrical and bell-shaped.\n\n\n\n\nset.seed(1) # for reproducibility\nsample1 &lt;- rep_sample_n(df, size = 100, reps = 10000, replace = TRUE)\n\ndefault is sampling with replacement, but can be changed with replace = FALSE\nsize is the number of samples to draw\nrep is the number of times to repeat the sampling process\n\n\n\n\npop_dist &lt;- multi_family_strata %&gt;%\n  ggplot(aes(x = current_land_value)) +\n  geom_histogram(bins = 50) +\n  xlab(\"current land value\") +\n  ggtitle(\"Population distribution\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#terms-and-definitions",
    "href": "block_2/552_stat_inter/552_stat_inter.html#terms-and-definitions",
    "title": "Statistical Inference",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\npoint_estimate\nA summary statistic calculated from a random sample that estimates an unknown population parameter of interest.\n\n\npopulation\nThe entire set of entities objects of interest.\n\n\npopulation_parameter\nA numerical summary value about the population.\n\n\nsample\nA collected subset of observations from a population.\n\n\nobservation\nA quantity or quality (or a set of these) from a single member of a population.\n\n\nsampling_distribution\nA distribution of point estimates, where each point estimate was calculated from a different random sample coming from the same population.\n\n\n\n\nTrue population parameter is denoted with Greek letters. (e.g. μ for mean)\nEstimated population parameter is denoted with Greek letters with a hat. (e.g. μ̂ for mean)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#types-of-statistical-questions",
    "href": "block_2/552_stat_inter/552_stat_inter.html#types-of-statistical-questions",
    "title": "Statistical Inference",
    "section": "",
    "text": "Type\nDescription\nExample\n\n\n\n\nDescriptive\nSummarizes a characteristic of a set of data without interpretation.\n- What is the frequency of bacterial illnesses in a data set?  - How many people live in each US state?\n\n\nExploratory\nAnalyzes data to identify patterns, trends, or relationships between variables.\n- Do certain diets correlate with bacterial illnesses?  - Does air pollution correlate with life expectancy in different US regions?\n\n\nInferential\nAnalyzes patterns, trends, or relationships in a representative sample to quantify applicability to the whole population. Estimation with associated randomness.\n- Is eating 5 servings of fruits and vegetables associated with fewer bacterial illnesses?  - Is gestational length different for first born babies?\n\n\nPredictive\nAims to predict measurements or labels for individuals. Not focused on causes but on predictions.\n- How many viral illnesses will someone have next year?  - What political party will someone vote for in the next US election?\n\n\nCausal\nInquires if changing one factor will change another factor in a population. Sometimes design allows for causal interpretation.\n- Does eating 5 servings of fruits and vegetables cause fewer bacterial illnesses?  - Does smoking lead to cancer?\n\n\nMechanistic\nSeeks to explain the underlying mechanism of observed patterns or relationships.\n- How do changes in diet reduce bacterial illnesses?  - How does airplane wing design affect air flow and decrease drag?\n\n\n\n\n\n\nDefine the population of interest.\nSelect the right sampling method according to the specific characteristics of our population of interest.\nSelect our sample size (Power Analysis).\nCollect the sampled data.\nMeasure and calculate the sample statistic.\nInfer the population value based on this sample statistic while accounting for sampling uncertainty."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#sampling",
    "href": "block_2/552_stat_inter/552_stat_inter.html#sampling",
    "title": "Statistical Inference",
    "section": "",
    "text": "Population\nSample\n\n\n\n\n\\(\\pi_E\\): population proportion\n\\(\\hat{\\pi}_E\\): sample proportion\n\n\n\\(\\mu\\): population mean\n\\(\\bar{\\mu}\\): sample mean\n\n\n\nPopulation Distribution:\n\nThe sample distribution is of a similar shape to the population distribution.\nThe sample point estimates are identical to the values for the true population parameter we are trying to estimate.\n\nSample Distribution of 1 Sample:\n\nTaking a random sample and calculating a point estimate is a “good guess” of the unknown population parameter you are interested in.\nAs the sample size increases:\n\nthe sampling distribution becomes narrower.\nmore sample point estimates are closer to the true population mean.\nthe sampling distribution appears more bell-shaped.\n\n\nSampling Distribution of Sample Means:\n\nThe sampling distribution is centered at the true population mean.\nMost sample means are at or very near the same value as the true population mean.\nThe sample distribution (if representative) is an estimate of the population distribution.\nThe sampling distribution of the sample means is not necessarily the same shape as the distribution of the population distribution and tends to be more symmetrical and bell-shaped."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#drawing-samples-in-r-rsamplerep_sample_n",
    "href": "block_2/552_stat_inter/552_stat_inter.html#drawing-samples-in-r-rsamplerep_sample_n",
    "title": "Statistical Inference",
    "section": "",
    "text": "set.seed(1) # for reproducibility\nsample1 &lt;- rep_sample_n(df, size = 100, reps = 10000, replace = TRUE)\n\ndefault is sampling with replacement, but can be changed with replace = FALSE\nsize is the number of samples to draw\nrep is the number of times to repeat the sampling process"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#plotting-histograms-in-r",
    "href": "block_2/552_stat_inter/552_stat_inter.html#plotting-histograms-in-r",
    "title": "Statistical Inference",
    "section": "",
    "text": "pop_dist &lt;- multi_family_strata %&gt;%\n  ggplot(aes(x = current_land_value)) +\n  geom_histogram(bins = 50) +\n  xlab(\"current land value\") +\n  ggtitle(\"Population distribution\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#more-on-bootstrapping",
    "href": "block_2/552_stat_inter/552_stat_inter.html#more-on-bootstrapping",
    "title": "Statistical Inference",
    "section": "More on Bootstrapping",
    "text": "More on Bootstrapping\n\nMean of the bootstrap sample is an estimate of the sample mean not the population mean. (unlike the sampling distribution of the sample mean)\nSpread of bootstrap sample is of similar shape to the sampling distribution of the sample mean.\n\nThis is because we used a bootstrap sample size that was the same as the original sample size.\nIf sample size is larger than original sample: underestimate the spread.\nThis is because the empirical sample distribution is an estimate of the population distribution."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#implementation-in-r",
    "href": "block_2/552_stat_inter/552_stat_inter.html#implementation-in-r",
    "title": "Statistical Inference",
    "section": "Implementation in R",
    "text": "Implementation in R\nset.seed(2485) # DO NOT CHANGE!\n\n# Take a sample from the population\nsample_1 &lt;- multi_family_strata |&gt;\n    rep_sample_n(size = 10) |&gt;\n\nbootstrap_means_10 &lt;- sample_1 |&gt;\n    ungroup() |&gt; # Remove grouping\n    # Removes the replicate column from rep_sample_n\n    select(current_land_value) |&gt; # Only select the column we want\n\n    # Bootstrap from the sample 2000 times\n    rep_sample_n(size = 10, reps = 2000, replace = T) |&gt;\n    # Calculate the mean for each bootstrap sample\n    group_by(replicate) |&gt;\n    summarise(mean_land_value = mean(current_land_value))\n\nbootstrap_means_10"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#calculating-confidence-intervals",
    "href": "block_2/552_stat_inter/552_stat_inter.html#calculating-confidence-intervals",
    "title": "Statistical Inference",
    "section": "Calculating Confidence Intervals",
    "text": "Calculating Confidence Intervals\n\nOne way : use the middle 95% of the distribution of bootstrap sample estimates to determine our endpoints. (2.5th and 97.5th percentiles)\nSpecific to a sample, not a population\nConfidence: yes or no on whether the interval contains the population parameter\nHigher sample size = narrower confidence interval\n\nIncreasing sample size increases the precision of our estimate\n\n\n\nWhat does 95% confidence Interval mean?\n\nIf we were to repeat this process over and over again and calculate the 95% CI many times,\n\n95% of the time expect true population parameter to be in the confidence interval\n\nThe confidence level is the percentage of time that the interval will contain the true population parameter if we were to repeat the process over and over again.\nOther confidence levels: 90%, 99%\n\nhigher confidence level, wider the interval = more likely to contain the population parameter - higher for use cases where we need to be more confident that the interval contains the population parameter (e.g. medical trials)\n\n\nUsing the null distribution, the \\(p\\)-value is the area to the right of \\(\\delta^*\\) and to the left of \\(-\\delta^*\\). In other words, the \\(p\\)-value is doubled for the two-tailed test. Conclusion: If we fail to reject the null hypothesis for a one-sided test, we would definitely not be able to reject it for a two-sided test."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#steps-to-calculate-a-confidence-interval",
    "href": "block_2/552_stat_inter/552_stat_inter.html#steps-to-calculate-a-confidence-interval",
    "title": "Statistical Inference",
    "section": "Steps to calculate a confidence interval",
    "text": "Steps to calculate a confidence interval\n\nCalculate the true population parameter (e.g. mean), normally we don’t know this\nGet a sample from a population of size n\n\nset.seed(552) # For reproducibility.\nsample &lt;- rep_sample_n(listings, size = 40)\n\nLook at the sample and decide on parameter of interest for distribution (e.g. median)\nBootstrap the sample m times (e.g. 10,000 times) with replacement from the existing sample and get the required statistic (e.g. median) for each bootstrap sample.\n\n\nmust be the same size as the original sample\n\nset.seed(552) # For reproducibility.\nbootstrap_estimates &lt;- sample %&gt;%\n  specify(response = room_type, success = \"Entire home/apt\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\")\nbootstrap_estimates\n\nGet the decided parameter of interest (e.g., median) for each of the bootstrap samples and make a distribution\nCalculate the confidence interval (e.g. 95%)\n\nuse infer::get_confidence_interval()\nreturns a tibble with the lower and upper bounds of the confidence interval\n\n\nget_confidence_interval(bootstrap_estimates, level = 0.90, type = \"percentile\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#fundamentals-of-hypothesis-testing",
    "href": "block_2/552_stat_inter/552_stat_inter.html#fundamentals-of-hypothesis-testing",
    "title": "Statistical Inference",
    "section": "Fundamentals of Hypothesis Testing",
    "text": "Fundamentals of Hypothesis Testing\n\nNull hypothesis: a statement of “no effect” or “no difference”\n\n\\(H_0: p_{control} - p_{variation} = \\delta = 0\\), where:\n\n\\(p_{control}\\): values of the control group\n\\(p_{variation}\\) values from the variation group\n\n\nAlternative hypothesis / \\(H_a\\): a statement of an effect or difference\n\n\\(H_a: p_{control} - p_{variation} = \\delta \\neq 0\\)\nclaim to seek statistical evidence for\n\nalpha: the probability of rejecting the null hypothesis when it is true, typically 0.05\n\nthe probability of a type I error\nthe probability of a false positive\n\\(\\alpha = P(\\text{reject } H_0 \\mid H_0 \\text{ is true})\\) \n\nProvided a strong enough statistical evidence, we can reject the null hypothesis and accept the alternative hypothesis\nObserved test statistic: \\(\\delta^*\\)\n\ne.g. \\(\\delta^* = \\hat{m}_{chinstrap} - \\hat{m}_{adelie}\\)\n\nwhere \\(\\hat{m}_{chinstrap}\\) is the sample mean (estimator) body mass of Chinstrap penguins"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#framework-for-hypothesis-testing-6-steps",
    "href": "block_2/552_stat_inter/552_stat_inter.html#framework-for-hypothesis-testing-6-steps",
    "title": "Statistical Inference",
    "section": "Framework for Hypothesis Testing (6 Steps)",
    "text": "Framework for Hypothesis Testing (6 Steps)\n\nDefine your null and alternative hypotheses.\n\nNull: The mean body mass of Chinstrap and Adelie are the same. \\(\\mu_{Chinstrap} - \\mu_{Adelie} = 0\\)\nAlt: The mean body mass of Chinstrap and Adelie are different. \\(\\mu_{Chinstrap} - \\mu_{Adelie} \\neq 0\\)\n\nwhere \\(\\mu_{Chinstrap}\\) is the population mean body mass of Chinstrap penguins\n\n\nCompute the observed test statistic coming from your original sample.\n\n\\(\\delta^* = \\hat{m}_{chinstrap} - \\hat{m}_{adelie}\\)\n\n\nchinstrap_adelie # data frame with chinstrap and adelie penguins only\n\nchinstrap_adelie_test_stat &lt;- chinstrap_adelie |&gt;\n  specify(formula = body_mass_g ~ species) |&gt;\n  calculate(\n  stat = \"diff in means\",\n  order = c(\"Chinstrap\", \"Adelie\")\n)\n\nSimulate the null hypothesis being true and calculate their corresponding test statistics.\n\ne.g. by randomly shuffling the data =&gt; any observed difference is due to chance\npermutation test: randomly shuffle the data and calculate the test statistic for each permutation (Without replacement)\nwould be a normal distribution about 0 (two-tailed test)\n\n\n# Running permutation test\nchinstrap_adelie_null_distribution &lt;- chinstrap_adelie |&gt;\n  specify(formula = body_mass_g ~ species) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(\n  stat = \"diff in means\",\n  order = c(\"Chinstrap\", \"Adelie\")\n  )\n\nGenerate the null distribution using these test statistics.\nObserve where the observed test statistic falls in the distribution\n\nif it falls in the extreme 5% of the distribution, we reject the null hypothesis\ni.e. if the p-value is less than \\(\\alpha\\), we reject the null hypothesis\n\nIf \\(\\delta\\) is near the extremes past some threshold defined with a significance level \\(\\alpha\\), we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#p-value",
    "href": "block_2/552_stat_inter/552_stat_inter.html#p-value",
    "title": "Statistical Inference",
    "section": "P-Value",
    "text": "P-Value\n\nthe probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true\nuse get_p_value function from infer R library\n\nchinstrap_adelie_p_value &lt;- chinstrap_adelie_null_distribution|&gt;\n  get_p_value(chinstrap_adelie_test_stat, direction = \"both\")\n\nResults:\n\nReject Null Hypothesis if p-value &lt; \\(\\alpha\\)\nFail to Reject Null Hypothesis if p-value &gt; \\(\\alpha\\)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#maximum-likelihood-estimation",
    "href": "block_2/552_stat_inter/552_stat_inter.html#maximum-likelihood-estimation",
    "title": "Statistical Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nDefinition:\n\nUsed to FIND estimators\nIDEA: Given a set of data and a statistical model (assumed pdf), MLE finds the parameter values that make the observed data most probable.\n\n\nSome key ideas:\n\nNeed to make distributional assumptions about the data, to specify the likelihood function\nNeed to consider nature (discrete/continuous) of the data\nGet the parameters that maximize the likelihood function\n\n\n\n\nLikelihood Function:\n\n\\(P(dataset | params) = l(\\lambda | y_1, y_2, ...)\\)\n\nprob mass/density function of the data given the parameters\n\npdf gives density not probability (area under curve == 1)\n\ngenerally: product of individual pdfs because iid\n\n\n\\[\\prod_{i=1}^n f_{Y_i}(y_i| \\lambda)\\]\n\nContext: Used when you have a specific set of outcomes (data) and you want to understand how likely different parameter values are, given that data.\n\nexample: Have a dataset of 3 obs (\\(y_1, y_2, y_3\\)) and want to know how likely it is that \\(\\lambda = 2\\). Assume exponential distribution.\n\\[l(\\lambda | y_1, y_2, y_3) = \\prod_{i=1}^3 f_{Y_i}(y_i| \\lambda) = \\prod_{i=1}^3 \\lambda e^{-\\lambda y_i}\\]\n\nin R: dexp(y, rate = lambda, log = FALSE)\n\n\n\nObjective of MLE:\n\nThe aim is to find the parameter values that maximize this likelihood function.\n\n\\[w \\in argmax\\{P(dataset | params)\\} = \\prod_{i=1}^3 f_{Y_i}(y_i| \\lambda)\\]\nbut values are very small, so we take the log of the likelihood function to simplify the math.\n\\[w \\in argmax\\{\\log(P(dataset | params))\\} = \\sum_{i=1}^n \\ln f_{Y_i}(y_i| \\lambda) \\]\n\n\nProcedure:\n\nChoose likelihood function\nTake the log of the likelihood function\nDifferentiate the log-likelihood function with respect to the parameters to get max\nSolve for the parameters\n\nin R can do this:\nexp_values &lt;- tibble(\n  possible_lambdas = seq(0.01, 0.2, 0.001),\n  likelihood = map_dbl(possible_lambdas, ~ prod(dexp(sample_n30$values, .))),\n  log_likelihood = map_dbl(possible_lambdas, ~ log(prod(dexp(sample_n30$values, .))))\n)\n\nR optim::optimize()\n\noptimize(f, interval, maximum = TRUE)\n\nf: function to be optimized\ninterval: vector of length 2 giving the start and end of the interval\nmaximum: logical, should the function be maximized?\n\ne.g.\n# Log-likelihood function\nLL &lt;- function(l) log(prod(dexp(sample$values, l)))\n\noptimize(LL, c(0.01, 0.2), maximum = TRUE)\n\n\n\n\nProperties of MLE:\n\nConsistency: As the sample size grows, the MLE converges in probability to the true parameter value.\nAsymptotic Normality: For many models, as the sample size grows large, the distribution of the MLE approaches a normal distribution.\nEfficiency: Among the class of consistent estimators, MLE often has the smallest variance (is the most efficient) under certain regularity conditions.\nMLE can be biased, but it is asymptotically unbiased (i.e. as the sample size increases, the bias goes to 0)\n\n\n\nLimitations:\n\nRequires a specified model for the underlying data distribution. If the model is incorrect, MLE can give biased estimates.\nComputation can be challenging, especially for complex models or when there’s no closed-form solution."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#central-limit-theorem",
    "href": "block_2/552_stat_inter/552_stat_inter.html#central-limit-theorem",
    "title": "Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nPopulation and Sampling\n\nLets say population of N samples, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nGet a sample of n iid random samples \\(X_1, X_2, ..., X_n\\) from the population.\n\n\\[ \\bar{X} = \\frac{1}{n} \\sum*{i=1}^n X_i \\] \\[ S^2 = \\frac{1}{n-1} \\sum*{i=1}^n (X_i - \\bar{X})^2 \\]\n\nn-1 is to make it unbiased\n\n\n\nDefinition:\n\\(\\bar{X} \\dot\\sim N(\\mu, \\frac{\\sigma^2}{n})\\) as \\(n \\rightarrow \\infty\\)\n\nThe sampling distribution of the sample mean of a population distribution converges to a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\) as the sample size \\(n\\) gets larger when we sample with replacement. \nNOTE: standard deviation of the sampling distribution of the sample mean is called the standard error of the sample mean.\n\n\n\nAssumptions\n\nThe sample size is large enough (at least 30)\n\nunless the population is normal\n\nThe sample is drawn from a population with a finite variance and mean.\nThe samples are iid (independent and identically distributed) from the population.\nEach category in the population should have a sufficient number of samples. (e.g. #success at least 10)\n\nWhen we do inference using the CLT, we required a large sample for two reasons:\n\nThe sampling distribution of \\(\\bar{X}\\) tends to be closer to normal when the sample size is large.\nThe calculated standard error is typically very accurate when using a large sample.\n\n\n\nConfidence Interval using CLT\n\\[ CI = Point \\; Estimate \\pm Margin \\; of \\; Error \\]\n\\[ MoE = Z_x \\times SE = Z_x \\times \\frac{\\sigma}{\\sqrt{n}} \\]\nwhere \\(Z_x\\) is the z-score for the desired confidence level.\n\n95% confidence level: \\(Z_x = Z_{0.025} = 1.96\\)\n\nrecal in R: qnorm(0.975) = 1.96\n\n\n\n\nConfidence Interval for Proportions\n\nsample proportion: \\(\\hat{p} = \\frac{\\sum{X_i}}{n}\\)\n\nit is a sample mean of 0 and 1\n\nRule of thumb:\n\nsuccess: at least 10\nfailure: at least 10\n\n\nCI for proportions:\n\\[ \\hat{p} \\pm Z_x \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\]\nwhere standard error is \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) since the variance of a bernoulli distribution is \\(p(1-p)\\)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#hypothesis-testing-via-normal-and-t-testing",
    "href": "block_2/552_stat_inter/552_stat_inter.html#hypothesis-testing-via-normal-and-t-testing",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing via normal and T-testing",
    "text": "Hypothesis Testing via normal and T-testing\n\nSimple confidence interval Test\n\nIf we want to check if mean of population is equal to a value\n\nCan reasonably conclude that the population mean is not equal to the value if the value is not in the confidence interval\nIf the value is in the confidence interval, we cannot conclude that the population mean is equal to the value\n\nNEED TO DO A HYPOTHESIS TEST\n\n\n\n\n\nHypothesis testing\n\nPermutation test (can work on any estimator)\nStudent’s t-test (only works on sample mean)\nProportion test (only works on sample proportion)\n\nNote: All hypothesis tests are done under the null hypothesis\n\n\nP-Value\n\nthe probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true\n\nsmall p-value: observed test statistic is unlikely under the null hypothesis\n\nuse get_p_value function from infer R library\nResults:\n\nReject Null Hypothesis if p-value &lt; \\(\\alpha\\)\nFail to Reject Null Hypothesis if p-value &gt; \\(\\alpha\\)\n\n\n\nR code examples\n\nget p-value of normal distribution: pnorm(Z, mean = 0, sd = 1, lower.tail = FALSE)\n\n\n\n\nPermutation Hypothesis test (6 Steps)\n\nDefine your estimator (mean, median, sd, etc.)\nDefine your null and alternative hypotheses. (population)\nCompute the observed test statistic (sample) coming from your original sample.\nSimulate the null hypothesis being true and calculate their corresponding test statistics.\n\ne.g. by randomly shuffling the data =&gt; any observed difference is due to chance\nwould be a normal distribution about 0 (two-tailed test)\n\nGenerate the null distribution using these test statistics.\nObserve where the observed test statistic falls in the distribution\n\nif it falls in the extreme 5% of the distribution, we reject the null hypothesis\ni.e. if the p-value is less than \\(\\alpha\\), we reject the null hypothesis\n\nIf \\(\\delta\\) is near the extremes past some threshold defined with a significance level \\(\\alpha\\), we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\n\n\nProportion test\n\nDefine test statistic \\(\\delta = p_{control} - p_{variation}\\)\n\n\nAlso define null and alternative hypothesis\n\n\nDefine theory-based CLT test statistic \\[Z = \\frac{\\hat{p}_{control} - \\hat{p}_{variation}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_C} + \\frac{1}{n_V})}}\\] where: \\[\\hat{p} = \\frac{\\sum_{i=1}^{n_C} X_{iC} + \\sum_{i=1}^{n_V} X_{iV}}{n_C + n_V}\\]\n\nnumer: sample stat + effect size (difference in proportions)\ndenom: standard error of 2 sample\n\nSimulates to Normal distribution with mean 0 and standard deviation\n\n\nFind p-value and see if it is less than \\(\\alpha\\)\nOr find the CI and see if Z is in the CI\n\nIf Z is in the CI, we fail to reject the null hypothesis\n\n\n\nproportion test in R\n\nuse prop.test() function in R\n\nx is the number of successes in Bernoulli trials\nn is the number of trials\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\ncorrect = FALSE means we are using the normal approximation\n\nif correct = TRUE, we are using the exact binomial test\n\n\n\nprop.test(x = click_summary$success, n = click_summary$n,\n  correct = FALSE, alternative = \"less\"\n)\n\n\n\nPearson’s Chi-Squared Test\n\nTo identify whether two categorical variables are independent or not\n\nSteps:\n\nMake a contingency table\n\nlibrary(janitor)\n\n# Makes table with webpage, n, and num_success as columns\ncont_table_AB &lt;- click_through %&gt;%\n  tabyl(webpage, click_target)\n\nDo some calulations, e.g. for 2x2 table:\n\n\\[\\chi^2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\nChi-Squared Test in R\nchisq.test(cont_table_AB, correct = FALSE)\nwhere:\n\n\nT Test\n\nUses the CLT to approximate the sampling distribution of the sample mean\n\nONLY works on sample mean (because CLT only works on sample mean)\nONLY works on continuous data (because CLT only works on continuous data)\ncan be any distribution, because CLT makes it normal\n\nBasically similar to normal test but we have small sample size\nDegrees of freedom = n - 1 where n is the sample size\n\nas df increases, the t-distribution approaches the standard normal distribution\n\nTest Statistic = (sample mean - population mean) / standard error\n\nstandard error = standard deviation / sqrt(n)\nmore formally: \\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]\n\nnumerator: observed difference in sample means\ndenominator: standard error of the sampling distribution of the two-sample difference in means for the sample size we collected.\n\n\nOne sample t-test\n\nNull hypothesis: the population mean is equal to a value \\(\\mu = \\mu_0\\)\nTest statistic: \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\]\n\ns : sample standard deviation\n\nIn R use t.test() function\n\nx is the sample data\nmu is the value of the population mean\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\ncorrect = FALSE means we are using the normal approximation\n\nif correct = TRUE, we are using the exact binomial test\n\n\n\n\n\nTwo sample t-test\n\nNull hypothesis: the population mean of two groups are equal \\(\\delta = \\mu_1 - \\mu_2 = 0\\)\nTest statistic (if equal variance, $ &lt; 2):\n\n\\[t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\delta_0}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\n\\(S_p\\) is the pooled standard deviation \\[S_p = \\sqrt{\\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}}\\]\ndegrees of freedom: \\(df = n_1 + n_2 - 2\\)\nin R, use t.test() function\n\nformula is the formula for the test statistic variable ~ group\ndata is the data frame\nmu hypothesis value of \\(\\delta\\)\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\nvar.equal = TRUE means we are assuming equal variance\n\nif var.equal = FALSE, we are assuming unequal variance\n\n\n\n\n\n\nComparison with simulation hypothesis test\n\n\n\n\n\n\n\n\n\nTest Type\nFactor\nImpact on Distribution / Statistic\nImpact on p-value\n\n\n\n\nSimulation Hypothesis\nSample Size\nIncrease =&gt; narrower null distribution\nDecrease\n\n\n\nSample Variance\nIncrease =&gt; increase simulated null variance\nIncrease\n\n\n\nEffect Size\nIncrease =&gt; more extreme test statistic\nIncrease (more likely to reject null)\n\n\nNormal/T-Testing\nEffect Size\nIn numerator of test statistic\nIncrease (if other factors remain same)\n\n\n\nSample Variance\nIncrease =&gt; increase standard error\nIncrease\n\n\n\nSample Size\nIncrease =&gt; decrease standard error\nDecrease"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#errors-in-inference",
    "href": "block_2/552_stat_inter/552_stat_inter.html#errors-in-inference",
    "title": "Statistical Inference",
    "section": "Errors in Inference",
    "text": "Errors in Inference\n\nTypes of Errors\n\nType I error: False positive $ P( H_0 | H_0 ) = $ \nType II error: False negative $ P( H_0 | H_0 ) = $\n\n\n\n\n\n\n\n\n\nDecision\nTrue Condition Positive\nTrue Condition Negative\n\n\n\n\nTest Positive\nCorrect (True Positive)\nType I Error (False Positive)\n\n\nTest Negative\nType II Error (False Negative)\nCorrect (True Negative)\n\n\n\n\n\nVisual representation of errors\n\nParameters:\n\n\\(\\beta\\), power = 1 - \\(\\beta\\)\n\\(\\alpha\\): sets type I error rate (how often we reject the null hypothesis when it is true)\ncohen’s d : effect size\n\n\\(d = \\frac{\\mu_1 - \\mu_2}{\\sigma}\\), where:\n\n\\(\\mu_1\\): mean of group 1\n\\(\\mu_2\\): mean of group 2\n\\(\\sigma\\): standard deviation of the population"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html",
    "href": "block_2/531_viz/531_viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Easier to understand data when it is visualized\nAnscombe’s quartet: 4 datasets with the same mean, variance, correlation, and linear regression line but look very different when plotted\nWe are using high-level declarative (altair, ggplot) instead of low-level imperative (matplotlib) \n\n\n\n\n\ngrammar: alt.Chart(data).mark_type().encode(x,y).properties()\nexample gallery: https://altair-viz.github.io/gallery/index.html\n\nimport altair as alt\n\n# to unlimit the number of rows displayed (5000 by default)\nalt.data_transformers.enable('vegafusion')\n\nalt.Chart(cars, title=\"My plot title\").mark_line(opacity=0.6).encode(\n    x=alt.X(\"Year:T\"), # encoders\n    y=alt.Y(\"mean(Miles_per_Gallon)\"),  # same as doing `y=cars.groupby('Year')['Miles_per_Gallon'].mean()`\n    color=alt.Color(\"Origin\"),\n).facet(\n    \"region\",\n    columns=2, # number of columns in the facet\n)\n\n\n\n\n\n\n\n\n\n\n\nMark Type\nDescription\nExample Use Case\n\n\n\n\nmark_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\nmark_bar\nBar charts.\nCompare discrete categories or time intervals.\n\n\nmark_circle, mark_square, mark_geoshape\nGeometric shape marks.\nVisualize point data in 2D space (circle, square) or geographic regions and shapes (geoshape).\n\n\nmark_line\nLine charts.\nShow trends over a continuous domain.\n\n\nmark_point\nScatter plots with customizable point shapes.\nVisualize point data with various symbols.\n\n\nmark_rect\nRectangular marks, used in heatmaps.\nDisplay data in 2D grid-like structures.\n\n\nmark_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\n\n\nno violin plot but can do mark_circle with y and color the same variable and size as count().\npandas explode df.explode() to explode a column of lists into multiple rows\n\n\n\n\nalt.X(\"Year:T\") is the same as alt.X(\"Year\", type=\"temporal\")\ncan do alt.X().scale() to change the scale of the axis\n\n.stack() to stack the axis\n.scale() to change the scale\n\n.scale(type=\"log\") to change to log scale\n.scale(range=(0, 100)) to change the domain\n.scale(zero=False) to not include 0 in the domain\n\n.bin() to bin the axis, makes histogram if used on mark_bar()\n\ndefault: stack=True\ntakes in binwidth or maxbins\n\n.sort() to sort the axis\n\n.sort(“x”) to sort by x (ascending)\n.sort(“-x”) to sort by x (descending)\n\n.title(\"\"): change the title of the axis\n\n\n\n\n\n\n\n\nData Type\nShorthand Code\nDescription\n\n\n\n\nquantitative\nQ\na continuous real-valued quantity\n\n\nordinal\nO\na discrete ordered quantity\n\n\nnominal\nN\na discrete unordered category\n\n\ntemporal\nT\na time or date value\n\n\ngeojson\nG\na geographic shape\n\n\n\nhttps://altair-viz.github.io/user_guide/encodings/index.html#encoding-data-types\n\n\n\nalt.Chart(movies).mark_point(opacity=0.3, size=10).encode(\n     alt.X(alt.repeat('row')).type('quantitative'),\n     alt.Y(alt.repeat('column')).type('quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=['runtime', 'revenue', 'budget'],\n    row=['runtime', 'revenue', 'budget']\n)\n\n\n\ncorr_df = (\n    movies\n    .corr('spearman', numeric_only=True)\n    .abs()                      # Use abs for negative correlation to stand out\n    .stack()                    # Get df into long format for altair\n    .reset_index(name='corr'))  # Name the index that is reset to avoid name collision\n\nalt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='corr',\n    color='corr'\n)\n\n\n\n\n\ngrammar: ggplot(data, aes(x, y)) + geom_type() + facet_wrap() + theme()\n\nread_csv(url) |&gt;\n    ggplot(aes(x = Year, y = Miles_per_Gallon, color = Origin)) +\n    geom_line(stat = \"summary\", fun = mean, alpha = 0.6) +\n    # stat_summary(geom = \"line\", fun = mean) # alternative way of doing the same thing\n    facet_wrap(~ region, ncol = 2) +\n    # properties\n    ggtitle(\"My plot title\") +\n    labs(x = \"Year\", y = \"Miles per Gallon\")\n\nin ggplot, fill and color are different\n\nfill is the color of the inside of the shape (bars, area, etc.)\ncolor is the color of the outline of the shape (points, lines, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nExample Use Case\n\n\n\n\ngeom_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\ngeom_bar & geom_col\nBar charts.\nCompare discrete categories. geom_col is a special case of geom_bar where height is pre-determined.\n\n\ngeom_point, geom_tile, geom_polygon\nGeometric shape marks.\ngeom_point for scatter plots, geom_tile for heatmaps, and geom_polygon for custom shapes.\n\n\ngeom_line\nLine charts.\nShow trends over a continuous domain.\n\n\ngeom_smooth\nAdds a smoothed conditional mean.\nFit and display a trend line for scattered data.\n\n\ngeom_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\ngeom_histogram\nHistograms.\nDisplay frequency distributions.\n\n\ngeom_text & geom_label\nDisplay text and labels.\nAnnotate plots with text or labeled rectangles.\n\n\ngeom_jitter\nPoints with a small amount of random noise.\nDisplay individual data points without overlap.\n\n\ngeom_path\nConnect observations in the order they appear.\nDisplay paths or trajectories.\n\n\ngeom_violin\nViolin plots.\nCombine boxplot and kernel density estimation.\n\n\ngeom_rug\nMarginal rug plots.\nDisplay 1D data distribution on plot margins.\n\n\n\n\n\n\n\nreorder() to reorder the axis\n\ngm2018 %&gt;%\n    add_count(region) %&gt;%\n    ggplot(aes(y = reorder(region, n))) + # y is region, reorder by its count (n)\n    geom_bar()\n\n\n\n\nRepeated Charts\n\nlibrary(GGally)\nGGally::ggpairs(movies %&gt;% select_if(is.numeric), progress = FALSE)\n\nCorrelation plot\n\nGGally::ggcorr(movies)\n\n\n\n\n\nFaceting displays groups based on a dataset variable in separate subplots.\nIt’s useful for splitting data over additional categorical variables.\nAvoids overloading a single chart with too much information.\nUsing color groupings in a histogram can obscure differences between groups.\nFaceting each group in separate subplots clarifies distribution comparisons.\n\n\n\n\n\n\n\n\n\n\n\nshows only 1 value of distribution, can show 3 with error bars:\n\nDot plots\nBar plots\n\n\n\n\n\nbox plots: normally shows 5 values (min, max, median, 25th percentile, 75th percentile)\nHistograms\n\ncons: binning required and distribution sensitive to bin settings\n\nKDE\n\nshows density of data points\ngives a smooth curve because uses “gaussian” (default) centered at each data point\nNo binning required (not sensitive to bin width or bin location)\nless affected by sparse data\ncons: y-axis is density, not count (hard to interpret)\n\nViolin plots: mirror of density estimates (KDE)\n\nmore would show higher density, unlike box plots (smaller box plots would show higher density)\ncons: over-smoothing of data (when too little data points)\n\n\n\n\n\n\nscatter plots: shows all data points (can overlap and saturate)\njitter plots: scatter plots with jittering over a small width in y-axis\nraincloud plots: violin plots with jittering\n\n\n\n\n\n\n\nalt.Chart(df).transform_density('x', as_=['x', 'density']).mark_area().encode(x='x:Q', y='density:Q')\nalt.Chart(df).explode('col_name'): explode a column (of list normally) into multiple rows"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#intro-to-data-visualization",
    "href": "block_2/531_viz/531_viz.html#intro-to-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Easier to understand data when it is visualized\nAnscombe’s quartet: 4 datasets with the same mean, variance, correlation, and linear regression line but look very different when plotted\nWe are using high-level declarative (altair, ggplot) instead of low-level imperative (matplotlib)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-in-python-altair",
    "href": "block_2/531_viz/531_viz.html#plotting-in-python-altair",
    "title": "Data Visualization",
    "section": "",
    "text": "grammar: alt.Chart(data).mark_type().encode(x,y).properties()\nexample gallery: https://altair-viz.github.io/gallery/index.html\n\nimport altair as alt\n\n# to unlimit the number of rows displayed (5000 by default)\nalt.data_transformers.enable('vegafusion')\n\nalt.Chart(cars, title=\"My plot title\").mark_line(opacity=0.6).encode(\n    x=alt.X(\"Year:T\"), # encoders\n    y=alt.Y(\"mean(Miles_per_Gallon)\"),  # same as doing `y=cars.groupby('Year')['Miles_per_Gallon'].mean()`\n    color=alt.Color(\"Origin\"),\n).facet(\n    \"region\",\n    columns=2, # number of columns in the facet\n)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#different-mark_types",
    "href": "block_2/531_viz/531_viz.html#different-mark_types",
    "title": "Data Visualization",
    "section": "",
    "text": "Mark Type\nDescription\nExample Use Case\n\n\n\n\nmark_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\nmark_bar\nBar charts.\nCompare discrete categories or time intervals.\n\n\nmark_circle, mark_square, mark_geoshape\nGeometric shape marks.\nVisualize point data in 2D space (circle, square) or geographic regions and shapes (geoshape).\n\n\nmark_line\nLine charts.\nShow trends over a continuous domain.\n\n\nmark_point\nScatter plots with customizable point shapes.\nVisualize point data with various symbols.\n\n\nmark_rect\nRectangular marks, used in heatmaps.\nDisplay data in 2D grid-like structures.\n\n\nmark_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\n\n\nno violin plot but can do mark_circle with y and color the same variable and size as count().\npandas explode df.explode() to explode a column of lists into multiple rows\n\n\n\n\nalt.X(\"Year:T\") is the same as alt.X(\"Year\", type=\"temporal\")\ncan do alt.X().scale() to change the scale of the axis\n\n.stack() to stack the axis\n.scale() to change the scale\n\n.scale(type=\"log\") to change to log scale\n.scale(range=(0, 100)) to change the domain\n.scale(zero=False) to not include 0 in the domain\n\n.bin() to bin the axis, makes histogram if used on mark_bar()\n\ndefault: stack=True\ntakes in binwidth or maxbins\n\n.sort() to sort the axis\n\n.sort(“x”) to sort by x (ascending)\n.sort(“-x”) to sort by x (descending)\n\n.title(\"\"): change the title of the axis\n\n\n\n\n\n\n\n\nData Type\nShorthand Code\nDescription\n\n\n\n\nquantitative\nQ\na continuous real-valued quantity\n\n\nordinal\nO\na discrete ordered quantity\n\n\nnominal\nN\na discrete unordered category\n\n\ntemporal\nT\na time or date value\n\n\ngeojson\nG\na geographic shape\n\n\n\nhttps://altair-viz.github.io/user_guide/encodings/index.html#encoding-data-types\n\n\n\nalt.Chart(movies).mark_point(opacity=0.3, size=10).encode(\n     alt.X(alt.repeat('row')).type('quantitative'),\n     alt.Y(alt.repeat('column')).type('quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=['runtime', 'revenue', 'budget'],\n    row=['runtime', 'revenue', 'budget']\n)\n\n\n\ncorr_df = (\n    movies\n    .corr('spearman', numeric_only=True)\n    .abs()                      # Use abs for negative correlation to stand out\n    .stack()                    # Get df into long format for altair\n    .reset_index(name='corr'))  # Name the index that is reset to avoid name collision\n\nalt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='corr',\n    color='corr'\n)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-in-r-ggplot2",
    "href": "block_2/531_viz/531_viz.html#plotting-in-r-ggplot2",
    "title": "Data Visualization",
    "section": "",
    "text": "grammar: ggplot(data, aes(x, y)) + geom_type() + facet_wrap() + theme()\n\nread_csv(url) |&gt;\n    ggplot(aes(x = Year, y = Miles_per_Gallon, color = Origin)) +\n    geom_line(stat = \"summary\", fun = mean, alpha = 0.6) +\n    # stat_summary(geom = \"line\", fun = mean) # alternative way of doing the same thing\n    facet_wrap(~ region, ncol = 2) +\n    # properties\n    ggtitle(\"My plot title\") +\n    labs(x = \"Year\", y = \"Miles per Gallon\")\n\nin ggplot, fill and color are different\n\nfill is the color of the inside of the shape (bars, area, etc.)\ncolor is the color of the outline of the shape (points, lines, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nExample Use Case\n\n\n\n\ngeom_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\ngeom_bar & geom_col\nBar charts.\nCompare discrete categories. geom_col is a special case of geom_bar where height is pre-determined.\n\n\ngeom_point, geom_tile, geom_polygon\nGeometric shape marks.\ngeom_point for scatter plots, geom_tile for heatmaps, and geom_polygon for custom shapes.\n\n\ngeom_line\nLine charts.\nShow trends over a continuous domain.\n\n\ngeom_smooth\nAdds a smoothed conditional mean.\nFit and display a trend line for scattered data.\n\n\ngeom_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\ngeom_histogram\nHistograms.\nDisplay frequency distributions.\n\n\ngeom_text & geom_label\nDisplay text and labels.\nAnnotate plots with text or labeled rectangles.\n\n\ngeom_jitter\nPoints with a small amount of random noise.\nDisplay individual data points without overlap.\n\n\ngeom_path\nConnect observations in the order they appear.\nDisplay paths or trajectories.\n\n\ngeom_violin\nViolin plots.\nCombine boxplot and kernel density estimation.\n\n\ngeom_rug\nMarginal rug plots.\nDisplay 1D data distribution on plot margins.\n\n\n\n\n\n\n\nreorder() to reorder the axis\n\ngm2018 %&gt;%\n    add_count(region) %&gt;%\n    ggplot(aes(y = reorder(region, n))) + # y is region, reorder by its count (n)\n    geom_bar()\n\n\n\n\nRepeated Charts\n\nlibrary(GGally)\nGGally::ggpairs(movies %&gt;% select_if(is.numeric), progress = FALSE)\n\nCorrelation plot\n\nGGally::ggcorr(movies)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#faceting",
    "href": "block_2/531_viz/531_viz.html#faceting",
    "title": "Data Visualization",
    "section": "",
    "text": "Faceting displays groups based on a dataset variable in separate subplots.\nIt’s useful for splitting data over additional categorical variables.\nAvoids overloading a single chart with too much information.\nUsing color groupings in a histogram can obscure differences between groups.\nFaceting each group in separate subplots clarifies distribution comparisons."
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#most-basic-fast-to-make",
    "href": "block_2/531_viz/531_viz.html#most-basic-fast-to-make",
    "title": "Data Visualization",
    "section": "",
    "text": "shows only 1 value of distribution, can show 3 with error bars:\n\nDot plots\nBar plots"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#a-bit-better",
    "href": "block_2/531_viz/531_viz.html#a-bit-better",
    "title": "Data Visualization",
    "section": "",
    "text": "box plots: normally shows 5 values (min, max, median, 25th percentile, 75th percentile)\nHistograms\n\ncons: binning required and distribution sensitive to bin settings\n\nKDE\n\nshows density of data points\ngives a smooth curve because uses “gaussian” (default) centered at each data point\nNo binning required (not sensitive to bin width or bin location)\nless affected by sparse data\ncons: y-axis is density, not count (hard to interpret)\n\nViolin plots: mirror of density estimates (KDE)\n\nmore would show higher density, unlike box plots (smaller box plots would show higher density)\ncons: over-smoothing of data (when too little data points)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#showing-all-data",
    "href": "block_2/531_viz/531_viz.html#showing-all-data",
    "title": "Data Visualization",
    "section": "",
    "text": "scatter plots: shows all data points (can overlap and saturate)\njitter plots: scatter plots with jittering over a small width in y-axis\nraincloud plots: violin plots with jittering"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-density",
    "href": "block_2/531_viz/531_viz.html#plotting-density",
    "title": "Data Visualization",
    "section": "",
    "text": "alt.Chart(df).transform_density('x', as_=['x', 'density']).mark_area().encode(x='x:Q', y='density:Q')\nalt.Chart(df).explode('col_name'): explode a column (of list normally) into multiple rows"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#some-notes",
    "href": "block_2/531_viz/531_viz.html#some-notes",
    "title": "Data Visualization",
    "section": "Some notes",
    "text": "Some notes\n\nsame data can give different messages\nkeep same color mapping for same categories across plots\nlabels horizontally &gt; vertically\nPie charts aint it when:\n\ncategories &gt; 3\ncomparing &gt; 1 pie charts\n\nDon’t have too many lines in a line chart\n\nunless you gray out the lines and highlight the one you want to show\n\n\n\nOverplotting\n\nToo many points in a plot\nSome solutions:\n\nlower size\nlower opacity (alpha in ggplot)\n\nIf you have too many points, you can use a heatmap\n\nggplot: gromm::geom_bin2d(bins=40) or gromm::geom_hex(bins=40)\n\nhex is preferred because better granularity\n\n\nmatch group colours in different charts\n\nggplot(diamonds) +\n    aes(x=carat, y=price) +\n    geom_hex(bins=40)\n\naltair:\n\nalt.Chart(diamonds).mark_rect().encode(\n  alt.X('carat', bin=alt.Bin(maxbins=40)),\n  alt.Y('price', binsalt.Bin(maxbins=40)),\n  alt.Color('count()'))\n\n\nAxes Formatting\n\nPython Altair:\n\n# clip=True to remove points outside of the plot axis\nalt.Chart(diamonds,\n        title='Diamonds',\n        subtitle='Price vs. Carat'\n    ).mark_rect(clip=True).encode(\n  alt.X('carat',\n    bin=alt.Bin(maxbins=40),\n    title='Carat', # set the axis title, blank if no title\n    scale=alt.Scale(domain=(0, 3)), # set the axis extent\n    reverse=True # reverse=True to flip the axis (largest value on the left)\n    # axis=None # remove the axis (no labels no titles)\n    ),\n  alt.Y('price',\n    binsalt.Bin(maxbins=40),\n    title='Price',\n    scale=alt.Scale(domain=(0, 2000)),\n    axis=alt.Axis(format='$s') # set the axis format 1000 -&gt; $1.0k\n    ),\n  alt.Color('count()'))\n\nR ggplot:\n\nggplot(diamonds) +\n    aes(x=carat, y=price) +\n    geom_hex(bins=40) +\n    scale_x_continuous(\n        limits=c(0, 3),\n        expand=c(0, 0)) + # expand=c(0, 0) to remove padding [c(mult, add)] for both sides\n    scale_y_continuous(\n        limits=c(0, 2000),\n        trans=\"reverse\", # to flip the axis\n        labels=scales::label_dollar()) + # to format the axis labels\n    labs(x=\"Carat\", y=\"Price\", fill=\"Number\", # to set the axis labels\n        title=\"Diamonds\", subtitle=\"Price vs. Carat\") + # to set the title and subtitle\n    theme(axis.title.x=element_blank(), # to remove the axis title\n        axis.text.x=element_blank(), # to remove the axis labels\n        axis.ticks.x=element_blank()) +# to remove the axis ticks\n    # theme_void() # to remove everything (no labels no titles)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#trendlines",
    "href": "block_2/531_viz/531_viz.html#trendlines",
    "title": "Data Visualization",
    "section": "Trendlines",
    "text": "Trendlines\n\n(Rolling) Mean: average of a subset of data\n\nto communicate to general public\n\nLinear Regression: line of best fit\n\nfor extrapolation\n\npoints +  points.mark_line(size=3).transform_regression(\n  'Year',\n  'Horsepower',\n  groupby=['Origin'],\n  method='poly', # 'linear' (default), 'log', 'exp', 'pow', 'quad', 'poly'\n)\nLoess: linear regression with a moving window (subset of data)\n\npython:\n\n\npoints +  points.mark_line(size=3).transform_loess(\n    'Year',\n    'Horsepower',\n    groupby=['Origin'],\n    bandwidth=0.8, # 0.3 (default), 1 (max is similar to linear regression)\n)\n\nR:\n\nggplot(cars) +\n    aes(x = Year,\n        y = Horsepower,\n        color = Origin,\n        fill = Origin) + # fill the CI area around the line\n    geom_point() +\n    geom_smooth(se = FALSE,\n        span = 0.8, # similar to bandwidth\n        method = \"loess\") # method = \"lm\" for linear regression"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#some-general-rules",
    "href": "block_2/531_viz/531_viz.html#some-general-rules",
    "title": "Data Visualization",
    "section": "Some general rules",
    "text": "Some general rules\n\nin Altair: `alt.Color(‘species’).scale(scheme=‘dark2’, reverse=True)\nin ggplot: scale_color_brewer(palette='Dark2')\n\n\n\n\n\n\n\n\n\n\nData Type\nVariation\nColor Map Type\nExample\n\n\n\n\nNumerical\nVary value\nSequential (Perceptually uniform)\nNumber of people (0-100)\n\n\nCategorical\nVary hue\nCategorical\nSubject (math, science, english)\n\n\nOrdered\nVary hue and value\nNot specified\nLow, medium, high\n\n\nCyclic\nVary hue and value\nCyclic\nDegrees (0-360) or days of week (0-6)\n\n\nData with natural center\nNot specified\nDiverging\nTemperature (0-100)\n\n\n\n \n \n\nAlso consider intuitive colors\n\n(e.g. red for hot, blue for cold)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#add-annotations",
    "href": "block_2/531_viz/531_viz.html#add-annotations",
    "title": "Data Visualization",
    "section": "Add annotations",
    "text": "Add annotations\n\nin Altair:\n\nbars = alt.Chart(wheat).mark_bar().encode(\n    x='year:O',\n    y=\"wheat\",\n    color=alt.Color('highlight').legend(None)\n)\nbars + bars.mark_text(dy=-5).encode(text='wheat')\n\n# Or for line plot\nlines = alt.Chart(stocks).mark_line().encode(\n    x='date',\n    y='price',\n    color=alt.Color('symbol').legend(None)\n)\n\ntext = alt.Chart(stock_order).mark_text(dx=20).encode(\n    x='date',\n    y='price',\n    text='symbol',\n    color='symbol'\n)\n\nlines + text\n\nin ggplot:\n\nggplot(wheat) +\n    aes(x = year,\n        y = wheat,\n        fill = highlight,\n        label = wheat) +\n    geom_bar(stat = 'identity', color = 'white') +\n    geom_text(vjust=-0.3)\n\n# Or for line plot\nggplot(stocks) +\n    aes(x = date,\n        y = price,\n        color = symbol,\n        label = symbol) +\n    geom_line() +\n    geom_text(data = stock_order, vjust=-1) +\n    ggthemes::scale_color_tableau() +\n    theme(legend.position = 'none')\n\nFrequency Framing\n\npeople normally judge probabilities wrongly\nNormalize the counts and show each individual count"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#error-bars",
    "href": "block_2/531_viz/531_viz.html#error-bars",
    "title": "Data Visualization",
    "section": "Error bars",
    "text": "Error bars\n\nSpecify what kind of error bar\n\nmin, max\nstd dev\nstd error\n95% confidence interval\n\nin Altair:\n\npoints.mark_errorband(extent='ci') # always 95% confidence interval\n\n# good way\nerr_bars = alt.Chart(cars).mark_errorbar(extent='ci', rule=alt.LineConfig(size=2)).encode(\n  x='Horsepower',\n  y='Origin'\n)\n\n(err_bars.mark_tick(color='lightgrey') + # show ticks\nerr_bars + # error bars\nerr_bars.mark_point(color='black').encode(x='mean(Horsepower)')) # mean as a point\n\nIn ggplot: use Hmisc::mean_cl_boot()\n\nggplot(cars) +\n    aes(x = Horsepower,\n        y = Origin) +\n    geom_point(shape = '|', color='grey', size=5) + # show ticks\n    geom_pointrange(stat = 'summary', fun.data = mean_cl_boot, size = 0.7) # error bar + mean as a point\n\n# For line mean and errobar\n... + geom_line(stat = 'summary', fun = mean) + # mean as a line\n    geom_ribbon(stat = 'summary', fun.data = mean_cl_boot, alpha=0.5, color = NA) # error bar as a ribbon"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#statistical-vs-practical-significance",
    "href": "block_2/531_viz/531_viz.html#statistical-vs-practical-significance",
    "title": "Data Visualization",
    "section": "Statistical vs practical significance",
    "text": "Statistical vs practical significance\n\nStatistical significance: is the difference between two groups statistically significant?\nPractical significance: is the difference between two groups practically significant?"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#figure-composition",
    "href": "block_2/531_viz/531_viz.html#figure-composition",
    "title": "Data Visualization",
    "section": "Figure composition",
    "text": "Figure composition\n\nPython\n\nvertically: plot1 & plot2 or alt.vconcat(plot1, plot2)\nhorizontally: plot1 | plot2 or alt.hconcat(plot1, plot2)\nadd title: (plot1 | plot2).properties(title='title')\n\n\n\nR\n\nuse package patchwork\nvertically: plot_grid(plot1, plot2, ncol=1)\nhorizontally: plot_grid(plot1, plot2, nrow=1)\nAdd labels: plot_grid(plot1, plot2, labels=c('A', 'B'))\nSet width: plot_grid(plot1, plot2, rel_widths=c(1, 2))"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#interactive-visualization",
    "href": "block_2/531_viz/531_viz.html#interactive-visualization",
    "title": "Data Visualization",
    "section": "Interactive visualization",
    "text": "Interactive visualization\n\nPanning and zooming\n\nAdd .interactive() to the end of the chart\n\nalt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n).interactive()\n\n\nDetails on demand\n\nAdd a tooltip to show details on demand\n\nalt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    tooltip=['Name', 'Origin']\n)\n\n\nInterval selection\n\nUse alt.selection_interval() to select a range of data points\n\nformat: alt.condition(check, if_true, if_false)\n\n\nbrush = alt.selection_interval(\n    encodings=['x'], # only select x axis, default is both x and y\n    resolve='union' # default is 'global', which means all charts are linked\n  )\n\npoints = alt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    # Use alt.condition to do a selection\n    color=alt.condition(brush, 'Origin', alt.value('lightgray'))\n).add_params(\n    brush\n)\n\n# linking different plots\npoints | points.encode(x='Acceleration')\n\n\nClick selection\n\ndefault: on='click'\n\nclick = alt.selection_point(fields=['Origin'], on='mouseover', bind='legend')\n\nbars = alt.Chart(cars).mark_bar().encode(\n    x='count()',\n    y='Origin',\n    color='Origin',\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).add_params(\n    click\n)\n\n\nFiltering data based on selection\n\nuse transform_filter to filter data based on selection\n\nbrush = alt.selection_interval()\nclick = alt.selection_point(fields=['Origin'], bind='legend')\n\npoints = alt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color=alt.condition(brush, 'Origin', alt.value('lightgray')),\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).add_params(\n    brush\n)\n\nbars = alt.Chart(cars).mark_bar().encode(\n    x='count()',\n    y='Origin',\n    color='Origin',\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).transform_filter( # changes bar plot based on selection of points\n    brush\n)\n\n(points & bars).add_params(click)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#pairwise-comparisons",
    "href": "block_2/531_viz/531_viz.html#pairwise-comparisons",
    "title": "Data Visualization",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\n\nPython:\n\npoints = alt.Chart(scores_this_year).mark_circle(size=50, color='black', opacity=1).encode(\n    alt.X('score_type'),\n    alt.Y('score'),\n    alt.Detail('time')).properties(width=300)\npoints.mark_line(size=1.8, opacity=0.8\n    ).encode(\n        alt.Color('diff',\n        scale=alt.Scale(scheme='blueorange', domain=(-6, 6))) # diverging colormap\n        # scale(range=['coral', 'green', 'steelblue']) # for categorial [negative, neutral, positive]\n    ) +\n    points\n\nR:\n\nggplot(scores_this_year) +\n    aes(x = score_type,\n        y = score,\n       group = time) +\n    geom_line(aes(color = self_belief), size = 0.8) +\n    geom_point(size=3) + labs(x='') +\n    # colour the lines by diverging colormap (sometimes not a good idea)\n    scale_color_distiller(palette = 'PuOr', limits = c(-5, 5))"
  },
  {
    "objectID": "list.html",
    "href": "list.html",
    "title": "List of Cheat Sheet",
    "section": "",
    "text": "Course Code.\nBlock #\nTitle\n\n\n\n\n511\n1\nIntro to Python\n\n\n521\n1\nPlatforms\n\n\n523\n1\nR Programming\n\n\n551\n1\nStatistics and Probability\n\n\n512\n2\nAlgorithms and Data Structures\n\n\n531\n2\nData Visualization\n\n\n552\n2\nStatistical Interference\n\n\n571\n2\nSupervised Learning"
  },
  {
    "objectID": "block_1/523_R/523_R.html",
    "href": "block_1/523_R/523_R.html",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "read_csv(url): reads a csv file\n\nread_csv(url, col_types = cols()): reads a csv file with no column types\nread_csv(url, col_types = cols(col_name = col_type)): reads a csv file with column types\nread_csv(url, skip = n, n_max = m): reads a csv file skipping n rows and reading m rows\n\nread_csv2(url): reads a csv file with a comma as decimal separator\nread_tsv(url): reads a tsv file\nread_delim(url, delim = \"\"): reads a file with a delimiter\n\n\n\n\n\nread_excel(file_path, sheet=\"name\"): reads an excel file\n\nto read url need to do download.file(url, destfile = \"file.xlsx\", mode = \"wb\")\n\n\n\n\n\n\nclean_names(df): cleans column names to match them with R conventions (e.g., col_name1)\n\n\n\n\n\nselect(df, col_name1, col_name2): selects cols\nfilter(df, col_name1 == \"value\", col_name2 &gt; 5): filters rows\n\nfilter(df, col_name1 %in% c(\"value1\", \"value2\")): filters if col_name1 is in a vector of values\n\narrange(df, col_name1): sorts rows, default is ascending\n\narrange(df, desc(col_name1)): sorts rows descending\n\nmutate(df, new_col_name = col_name1 + col_name2): creates new cols\nslice(df, 1:10): selects rows\n\nslice(df, 1): selects first row\n\npull(df, col_name1): extracts a column as a vector\n\n\n\n\n\nstr_detect(df$col_name, \"value\"): detects if a string contains a value\nstr_subset(df$col_name, \"value\"): subsets a string if it contains a value\nstr_split(df$col_name, \"value\"): splits a string by a value\n\nstr_split_fixed(df$col_name, \"value\", n): splits a string by a value and returns n columns (gets character matrix)\n\nseparate(df, col_name, into = c(\"new_col_name1\", \"new_col_name2\"), sep = \"value\"): separates a column into two columns\nstr_length(df$col_name): gets length of string\nstr_sub(df$col_name, start = n, end = m): gets substring from n to m\nstr_c(df$col_name1, df$col_name2, sep = \"value\"): concatenates two strings\n\nstr_c(df$col_name1, sep = \"value\", collapse = \"value\"): concatenates vector of string and collapses them into one string\n\nstr_replace(df$col_name, \"value\", \"new_value\"): replaces a value in a string\n\n\n\n\n\nfct_drop(df$col_name): drops unused levels\nfct_infreq(df$col_name): orders levels by frequency\nfct_reorder(df$col_name, df$col_name2): orders levels by another column\nfct_relevel(df$col_name, \"value\"): moves a level to the front\nfct_rev(df$col_name): reverses order of levels\n\n\n\n\n\npivot_longer(df, cols = c(col_name1, col_name2), names_to = \"new_col_name\", values_to = \"new_col_name\"): pivots cols to rows\npivot_wider(df, names_from = \"col_name1\", values_from = \"col_name2\"): pivots rows to cols\n\n\n\n\nCriteria:\n\nEach row is a single observation\nEach variable is a single column\nEach value is a single cell\n\n\n\n\n\nwe use &lt;- to assign values to variables.\nThis is because when we do median(x &lt;- c(1, 2, 3)) x is assigned to c(1, 2, 3) globally. \n\n\n\nobjects that contain 1 or more elements of the same type\nelements are ordered\nHeirarchy for coercion: character &gt; double &gt; integer &gt; logical\nto change type of vector use as.character(), as.double(), as.integer(), as.logical()\nto check if vector is of a certain type use is.character(), is.double(), is.integer(), is.logical()\nto check length of vector use length()\nto check type of vector use typeof()\n\nCan get vector from df using: df$col_name\n\n\n\nname &lt;- c(\"a\", \"b\", \"c\")\nname[1] # \"a\"\nname[2:3] # \"b\" \"c\"\nname[-1] # \"b\" \"c\"\nname[length(name)] # \"c\"]\n\n# Also...\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\ny[3] &lt;- 4\ny\n#&gt; [1] 1 2 4\n\n\n\n\n\nTibles inherit from data frames but are more strict. They are more consistent and have better printing.\nImportant properties:\n\nTibbles only output first 10 rows and all columns that fit on screen\nwhen you subset a tibble you always get a tibble, in data frames you get a vector\n\n\n\n\nuses lubridate package\n\ntoday(): gets today’s date, class is Date\nnow(): gets today’s date and time, class is POSIXct\nymd(), ydm(), mdy(), myd(), dmy(), dym(): converts character to date\nymd_hms(): converts character to date and time\nCan mutate date: dates |&gt; mutate = make_date(year, month, day)\nwdays(): gets day of week\n\nwdays(date, label = TRUE): gets abbreviated day of week (e.g., Mon)\nwday(date, label = TRUE, abbr = FALSE): gets day of week as full name (e.g., Monday)\n\nmdays(): gets day of month\nydays(): gets day of year\n\n\n\n\n\nbind_rows(df1, df2): binds rows of two dfs\nbind_cols(df1, df2): binds cols of two dfs\ninner_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, only keeps rows that match\nleft_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from df1\nsemi_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, only keeps rows that match\nanti_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, keeps only rows that don’t match\nfull_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from both dfs\n\n\n\n\n\n\n\ncase_when(): selectively modify column values based on conditions\n\ngapminder |&gt;\n    mutate(country = case_when(country == \"Cambodia\" ~ \"Kingdom of Cambodia\",\n    # only work if country is character (not factor)\n                            TRUE ~ country))\n\n# For multiple values\ngapminder |&gt;\n    mutate(continent = case_when(continent == \"Asia\" ~ \"Asie\",\n                                 continent == \"Europe\" ~ \"L'Europe\",\n                                 continent == \"Africa\" ~ \"Afrique\",\n                                 TRUE ~ continent)) #This is to keep the original value (not NA)\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ndrop_na()\nRemove rows based on NA in cols x to y\ndf %&gt;% drop_na(x:y)\n\n\n\nRemove rows if any column has NA\ndf %&gt;% drop_na()\n\n\n\n\n\n\n\n\nsummarise() or summarize(): returns a single value for each group\ngroup_by(): groups rows by a column\n\n# calculate the average life expectancy for the entire dataset\ngapminder |&gt;\n    summarise(mean_life_exp = mean(lifeExp))\n\n# calculate the average life expectancy for each continent and year\ngapminder |&gt;\n    group_by(continent, year) |&gt;\n    summarise(mean_life_exp = mean(lifeExp, na.rm = TRUE))\n    # na.rm = TRUE removes NAs from calculation\n\n# does not collapse the data frame, compute with group\ngapminder %&gt;%\n    group_by(country) %&gt;%\n    mutate(life_exp_gain = lifeExp - first(lifeExp)) %&gt;%\n    # first() returns the first value of a vector\n    head()\n\n\n\n\n\nmap(df, mean, na.rm = TRUE): retuirns a list\n\nna.rm = TRUE removes NAs from calculation\n\nmap_dfc(df, median): returns a tibble\nmap_dbl(df, max): returns a double vector\n\nCan use anonymous functions with map():\n# Long form\nmap_*(data, function(arg) function_being_called(arg, other_arg))\n# e.g.\nmap_df(data_entry, function(vect) str_replace(vect, pattern = \"Cdn\", replacement = \"Canadian\"))\n\n# short form\nmap_*(data, ~ function_being_called(., other_arg))\n# e.g.\nmap_df(data_entry, ~str_replace(., pattern = \"Cdn\", replacement = \"Canadian\"))\n\n\n\n\nHas roxygen comments, same as python docstrings\n\n#' Calculates the variance of a vector of numbers.\n#'\n#' Calculates the sample variance of data generated from a normal/Gaussian distribution,\n#' omitting NA's in the data.\n#'\n#' @param data numeric vector of numbers whose length is &gt; 1.\n#'\n#' @return numeric vector of length one, the variance.\n#'\n#' @examples\n#' variance(c(1, 2, 3))\nvariance &lt;- function(observations) {\n  if (!is.numeric(observations)) {\n    # Throws an error\n    stop(\"All inputs must be numeric.\")\n  }\n  sum((observations - mean(observations)) ^ 2) / (length(observations) - 1)\n}\n\nName Masking: if a variable is defined in the function, it will be used instead of the global variable\n\nif not in function, looks one level up, until it reaches the global environment\n\nR looks for values when the function is run, not when it is defined\nEach run is independent of the other\nLazy Evaluation: R only evaluates the arguments that are needed\n\nforce() forces R to evaluate an argument\n\n\n\n\n\ntest_that(\"Message to print if test fails\", expect_*(...))\n\ntest_that('variance expects a numeric vector', {\n    expect_error(variance(list(1, 2, 3)))\n    expect_error(variance(data.frame(1, 2, 3)))\n    expect_error(variance(c(\"one\", \"two\", \"three\")))\n})\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_identical\nTest two objects for being exactly equal\n\n\nexpect_equal\nCompare R objects x and y testing ‘near equality’ (can set a tolerance)\n\n\n\n- expect_equal(x, y, tolerance = 0.00001)\n\n\nexpect_equivalent\nCompare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\nexpect_error\nTests if an expression throws an error\n\n\nexpect_warning\nTests whether an expression outputs a warning\n\n\nexpect_output\nTests that print output matches a specified value\n\n\nexpect_true\nTests if the object returns TRUE\n\n\nexpect_false\nTests if the object returns FALSE\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsource(\"path/to/script.R\")\nTake functions from another script\n\n\nlibrary(package_name)\nImport a package\n\n\n\n\n\n(function(x) x + 1)(1) (see purrr package for examples)\n\n\n\n\n# create a nested data frame\nby_country &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    nest() # turns all other columns into a column called data (list of data frames)\nCommon workflow:\n\ngroup_by() + nest() to create a nested data frame\nmutate() + map() to add new columns\nunnest() to return to a regular data frame\n\nweather |&gt;\n# step 1\n  group_by(origin, month) |&gt;\n  nest() |&gt;\n# step 2\n  mutate(min_temp = map_dbl(data, ~min(.$temp, na.rm = T)),\n         max_temp = map_dbl(data, ~max(.$temp, na.rm = T)),\n         avg_temp = map_dbl(data, ~mean(.$temp, na.rm = T)))\n# step 3\nunnest(avg_temp) # only unnest if we get some intermediate list-columns from map\nAlternative to above code:\nweather_nested_2 &lt;- weather |&gt;\n  group_by(origin, month) |&gt;\n  summarise(min_temp = min(temp, na.rm = T),\n            max_temp = max(temp, na.rm = T),\n            avg_temp = mean(temp, na.rm = T))\n\n\n\nmetaprogramming: writing code that writes code\nWith tidyverse, they have a feautre called “non-standard evaluation” (NSE). Part of this is data masking.\n\nData Masking: data frame is promised to be first argument (data mask)\n\ncolumns act as if they are variables, filter(gapminder, country == \"Canada\", year == 1952)\nchecks dataframe first before global environment\n\nDelay in Evaluation: expressions are captured and evaluated later\nenquo(): quotes the argument\nsym(): turns column name into a function as a string\n!!: unquotes the argument\n{{ arg_name }}: unquotes and quotes the argument\n:=: Walrus operator - needed when assigning values\n\n# e.g.\nfilter_gap &lt;- function(col, val) {\n    col &lt;- enquo(col)\n    filter(gapminder, !!col == val)\n}\n\n# better way\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, {{col}} == val)\n}\n\nfilter_gap(country, \"Canada\")\n# e.g. of walrus operator\nfunction(data, group, col, fun) {\n    data %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise( {{ col }} := fun({{ col }}))\n}\n\n\n\nif passing varibales to tidyverse functions, use ...\n\nwhen variable not used in logical comparisons or variable assignment\n\nshould be last argument in function\ncan add multiple arguments\n\n\n\nsort_gap &lt;- function(x, ...) {\n    print(x + 1)\n    arrange(gapminder, ...)\n}\n\nsort_gap(1, year, continent, country)"
  },
  {
    "objectID": "block_1/523_R/523_R.html#r-packages",
    "href": "block_1/523_R/523_R.html#r-packages",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "read_csv(url): reads a csv file\n\nread_csv(url, col_types = cols()): reads a csv file with no column types\nread_csv(url, col_types = cols(col_name = col_type)): reads a csv file with column types\nread_csv(url, skip = n, n_max = m): reads a csv file skipping n rows and reading m rows\n\nread_csv2(url): reads a csv file with a comma as decimal separator\nread_tsv(url): reads a tsv file\nread_delim(url, delim = \"\"): reads a file with a delimiter\n\n\n\n\n\nread_excel(file_path, sheet=\"name\"): reads an excel file\n\nto read url need to do download.file(url, destfile = \"file.xlsx\", mode = \"wb\")\n\n\n\n\n\n\nclean_names(df): cleans column names to match them with R conventions (e.g., col_name1)\n\n\n\n\n\nselect(df, col_name1, col_name2): selects cols\nfilter(df, col_name1 == \"value\", col_name2 &gt; 5): filters rows\n\nfilter(df, col_name1 %in% c(\"value1\", \"value2\")): filters if col_name1 is in a vector of values\n\narrange(df, col_name1): sorts rows, default is ascending\n\narrange(df, desc(col_name1)): sorts rows descending\n\nmutate(df, new_col_name = col_name1 + col_name2): creates new cols\nslice(df, 1:10): selects rows\n\nslice(df, 1): selects first row\n\npull(df, col_name1): extracts a column as a vector\n\n\n\n\n\nstr_detect(df$col_name, \"value\"): detects if a string contains a value\nstr_subset(df$col_name, \"value\"): subsets a string if it contains a value\nstr_split(df$col_name, \"value\"): splits a string by a value\n\nstr_split_fixed(df$col_name, \"value\", n): splits a string by a value and returns n columns (gets character matrix)\n\nseparate(df, col_name, into = c(\"new_col_name1\", \"new_col_name2\"), sep = \"value\"): separates a column into two columns\nstr_length(df$col_name): gets length of string\nstr_sub(df$col_name, start = n, end = m): gets substring from n to m\nstr_c(df$col_name1, df$col_name2, sep = \"value\"): concatenates two strings\n\nstr_c(df$col_name1, sep = \"value\", collapse = \"value\"): concatenates vector of string and collapses them into one string\n\nstr_replace(df$col_name, \"value\", \"new_value\"): replaces a value in a string\n\n\n\n\n\nfct_drop(df$col_name): drops unused levels\nfct_infreq(df$col_name): orders levels by frequency\nfct_reorder(df$col_name, df$col_name2): orders levels by another column\nfct_relevel(df$col_name, \"value\"): moves a level to the front\nfct_rev(df$col_name): reverses order of levels\n\n\n\n\n\npivot_longer(df, cols = c(col_name1, col_name2), names_to = \"new_col_name\", values_to = \"new_col_name\"): pivots cols to rows\npivot_wider(df, names_from = \"col_name1\", values_from = \"col_name2\"): pivots rows to cols\n\n\n\n\nCriteria:\n\nEach row is a single observation\nEach variable is a single column\nEach value is a single cell"
  },
  {
    "objectID": "block_1/523_R/523_R.html#assignment-environment",
    "href": "block_1/523_R/523_R.html#assignment-environment",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "we use &lt;- to assign values to variables.\nThis is because when we do median(x &lt;- c(1, 2, 3)) x is assigned to c(1, 2, 3) globally. \n\n\n\nobjects that contain 1 or more elements of the same type\nelements are ordered\nHeirarchy for coercion: character &gt; double &gt; integer &gt; logical\nto change type of vector use as.character(), as.double(), as.integer(), as.logical()\nto check if vector is of a certain type use is.character(), is.double(), is.integer(), is.logical()\nto check length of vector use length()\nto check type of vector use typeof()\n\nCan get vector from df using: df$col_name\n\n\n\nname &lt;- c(\"a\", \"b\", \"c\")\nname[1] # \"a\"\nname[2:3] # \"b\" \"c\"\nname[-1] # \"b\" \"c\"\nname[length(name)] # \"c\"]\n\n# Also...\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\ny[3] &lt;- 4\ny\n#&gt; [1] 1 2 4"
  },
  {
    "objectID": "block_1/523_R/523_R.html#tibbles-vs-data-frames",
    "href": "block_1/523_R/523_R.html#tibbles-vs-data-frames",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Tibles inherit from data frames but are more strict. They are more consistent and have better printing.\nImportant properties:\n\nTibbles only output first 10 rows and all columns that fit on screen\nwhen you subset a tibble you always get a tibble, in data frames you get a vector"
  },
  {
    "objectID": "block_1/523_R/523_R.html#dates-and-times",
    "href": "block_1/523_R/523_R.html#dates-and-times",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "uses lubridate package\n\ntoday(): gets today’s date, class is Date\nnow(): gets today’s date and time, class is POSIXct\nymd(), ydm(), mdy(), myd(), dmy(), dym(): converts character to date\nymd_hms(): converts character to date and time\nCan mutate date: dates |&gt; mutate = make_date(year, month, day)\nwdays(): gets day of week\n\nwdays(date, label = TRUE): gets abbreviated day of week (e.g., Mon)\nwday(date, label = TRUE, abbr = FALSE): gets day of week as full name (e.g., Monday)\n\nmdays(): gets day of month\nydays(): gets day of year"
  },
  {
    "objectID": "block_1/523_R/523_R.html#joining-data",
    "href": "block_1/523_R/523_R.html#joining-data",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "bind_rows(df1, df2): binds rows of two dfs\nbind_cols(df1, df2): binds cols of two dfs\ninner_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, only keeps rows that match\nleft_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from df1\nsemi_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, only keeps rows that match\nanti_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, keeps only rows that don’t match\nfull_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from both dfs"
  },
  {
    "objectID": "block_1/523_R/523_R.html#change-or-remove-specific-values",
    "href": "block_1/523_R/523_R.html#change-or-remove-specific-values",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "case_when(): selectively modify column values based on conditions\n\ngapminder |&gt;\n    mutate(country = case_when(country == \"Cambodia\" ~ \"Kingdom of Cambodia\",\n    # only work if country is character (not factor)\n                            TRUE ~ country))\n\n# For multiple values\ngapminder |&gt;\n    mutate(continent = case_when(continent == \"Asia\" ~ \"Asie\",\n                                 continent == \"Europe\" ~ \"L'Europe\",\n                                 continent == \"Africa\" ~ \"Afrique\",\n                                 TRUE ~ continent)) #This is to keep the original value (not NA)\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ndrop_na()\nRemove rows based on NA in cols x to y\ndf %&gt;% drop_na(x:y)\n\n\n\nRemove rows if any column has NA\ndf %&gt;% drop_na()"
  },
  {
    "objectID": "block_1/523_R/523_R.html#iterate-over-groups-of-rows",
    "href": "block_1/523_R/523_R.html#iterate-over-groups-of-rows",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "summarise() or summarize(): returns a single value for each group\ngroup_by(): groups rows by a column\n\n# calculate the average life expectancy for the entire dataset\ngapminder |&gt;\n    summarise(mean_life_exp = mean(lifeExp))\n\n# calculate the average life expectancy for each continent and year\ngapminder |&gt;\n    group_by(continent, year) |&gt;\n    summarise(mean_life_exp = mean(lifeExp, na.rm = TRUE))\n    # na.rm = TRUE removes NAs from calculation\n\n# does not collapse the data frame, compute with group\ngapminder %&gt;%\n    group_by(country) %&gt;%\n    mutate(life_exp_gain = lifeExp - first(lifeExp)) %&gt;%\n    # first() returns the first value of a vector\n    head()"
  },
  {
    "objectID": "block_1/523_R/523_R.html#purrr-package",
    "href": "block_1/523_R/523_R.html#purrr-package",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "map(df, mean, na.rm = TRUE): retuirns a list\n\nna.rm = TRUE removes NAs from calculation\n\nmap_dfc(df, median): returns a tibble\nmap_dbl(df, max): returns a double vector\n\nCan use anonymous functions with map():\n# Long form\nmap_*(data, function(arg) function_being_called(arg, other_arg))\n# e.g.\nmap_df(data_entry, function(vect) str_replace(vect, pattern = \"Cdn\", replacement = \"Canadian\"))\n\n# short form\nmap_*(data, ~ function_being_called(., other_arg))\n# e.g.\nmap_df(data_entry, ~str_replace(., pattern = \"Cdn\", replacement = \"Canadian\"))"
  },
  {
    "objectID": "block_1/523_R/523_R.html#functions",
    "href": "block_1/523_R/523_R.html#functions",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Has roxygen comments, same as python docstrings\n\n#' Calculates the variance of a vector of numbers.\n#'\n#' Calculates the sample variance of data generated from a normal/Gaussian distribution,\n#' omitting NA's in the data.\n#'\n#' @param data numeric vector of numbers whose length is &gt; 1.\n#'\n#' @return numeric vector of length one, the variance.\n#'\n#' @examples\n#' variance(c(1, 2, 3))\nvariance &lt;- function(observations) {\n  if (!is.numeric(observations)) {\n    # Throws an error\n    stop(\"All inputs must be numeric.\")\n  }\n  sum((observations - mean(observations)) ^ 2) / (length(observations) - 1)\n}\n\nName Masking: if a variable is defined in the function, it will be used instead of the global variable\n\nif not in function, looks one level up, until it reaches the global environment\n\nR looks for values when the function is run, not when it is defined\nEach run is independent of the other\nLazy Evaluation: R only evaluates the arguments that are needed\n\nforce() forces R to evaluate an argument\n\n\n\n\n\ntest_that(\"Message to print if test fails\", expect_*(...))\n\ntest_that('variance expects a numeric vector', {\n    expect_error(variance(list(1, 2, 3)))\n    expect_error(variance(data.frame(1, 2, 3)))\n    expect_error(variance(c(\"one\", \"two\", \"three\")))\n})\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_identical\nTest two objects for being exactly equal\n\n\nexpect_equal\nCompare R objects x and y testing ‘near equality’ (can set a tolerance)\n\n\n\n- expect_equal(x, y, tolerance = 0.00001)\n\n\nexpect_equivalent\nCompare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\nexpect_error\nTests if an expression throws an error\n\n\nexpect_warning\nTests whether an expression outputs a warning\n\n\nexpect_output\nTests that print output matches a specified value\n\n\nexpect_true\nTests if the object returns TRUE\n\n\nexpect_false\nTests if the object returns FALSE"
  },
  {
    "objectID": "block_1/523_R/523_R.html#importing-packages-or-scripts",
    "href": "block_1/523_R/523_R.html#importing-packages-or-scripts",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\nsource(\"path/to/script.R\")\nTake functions from another script\n\n\nlibrary(package_name)\nImport a package\n\n\n\n\n\n(function(x) x + 1)(1) (see purrr package for examples)"
  },
  {
    "objectID": "block_1/523_R/523_R.html#nested-data-frames",
    "href": "block_1/523_R/523_R.html#nested-data-frames",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "# create a nested data frame\nby_country &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    nest() # turns all other columns into a column called data (list of data frames)\nCommon workflow:\n\ngroup_by() + nest() to create a nested data frame\nmutate() + map() to add new columns\nunnest() to return to a regular data frame\n\nweather |&gt;\n# step 1\n  group_by(origin, month) |&gt;\n  nest() |&gt;\n# step 2\n  mutate(min_temp = map_dbl(data, ~min(.$temp, na.rm = T)),\n         max_temp = map_dbl(data, ~max(.$temp, na.rm = T)),\n         avg_temp = map_dbl(data, ~mean(.$temp, na.rm = T)))\n# step 3\nunnest(avg_temp) # only unnest if we get some intermediate list-columns from map\nAlternative to above code:\nweather_nested_2 &lt;- weather |&gt;\n  group_by(origin, month) |&gt;\n  summarise(min_temp = min(temp, na.rm = T),\n            max_temp = max(temp, na.rm = T),\n            avg_temp = mean(temp, na.rm = T))"
  },
  {
    "objectID": "block_1/523_R/523_R.html#tidy-evaluation",
    "href": "block_1/523_R/523_R.html#tidy-evaluation",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "metaprogramming: writing code that writes code\nWith tidyverse, they have a feautre called “non-standard evaluation” (NSE). Part of this is data masking.\n\nData Masking: data frame is promised to be first argument (data mask)\n\ncolumns act as if they are variables, filter(gapminder, country == \"Canada\", year == 1952)\nchecks dataframe first before global environment\n\nDelay in Evaluation: expressions are captured and evaluated later\nenquo(): quotes the argument\nsym(): turns column name into a function as a string\n!!: unquotes the argument\n{{ arg_name }}: unquotes and quotes the argument\n:=: Walrus operator - needed when assigning values\n\n# e.g.\nfilter_gap &lt;- function(col, val) {\n    col &lt;- enquo(col)\n    filter(gapminder, !!col == val)\n}\n\n# better way\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, {{col}} == val)\n}\n\nfilter_gap(country, \"Canada\")\n# e.g. of walrus operator\nfunction(data, group, col, fun) {\n    data %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise( {{ col }} := fun({{ col }}))\n}\n\n\n\nif passing varibales to tidyverse functions, use ...\n\nwhen variable not used in logical comparisons or variable assignment\n\nshould be last argument in function\ncan add multiple arguments\n\n\n\nsort_gap &lt;- function(x, ...) {\n    print(x + 1)\n    arrange(gapminder, ...)\n}\n\nsort_gap(1, year, continent, country)"
  },
  {
    "objectID": "block_1/551_stats-and-prob/551_stats.html",
    "href": "block_1/551_stats-and-prob/551_stats.html",
    "title": "Statistics and Probability Cheat Sheet",
    "section": "",
    "text": "Statistics and Probability Cheat Sheet\n\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;MDS cheatsheets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;MDS cheatsheets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:List of Cheat Sheet\"&gt;List of Cheat Sheet&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/list.html\"&gt;/list.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;MDS cheatsheets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  }
]